# 5.使用 Wget 获得更多

我经常发现自己需要从网页上下载一个 tarball 或者一个配置文件到服务器上。或者在脚本中，我需要知道我可以通过 HTTP(S)查询本地和远程 web 服务器，以检查正常运行时间或某些内容的存在。最流行的功能丰富的工具之一是 Wget，我猜它代表“web get”。以前被称为 Geturl，Wget 被描述为“非交互式网络下载器”。它拥有几个支持的协议，即 HTTP，HTTPS 和 FTP，也有能力看过去和查询一个网站以外的 HTTP 代理；确实是一个强大的功能集。但是你能用它做什么呢？在这一章中，你将探索它的一些特性，从几个简单的例子开始，继续一些你可能会发现 Wget 最有用的场景。

要在基于 Debian 和 Ubuntu 的系统上安装 Wget，运行下面的命令。由于它的流行，很有可能已经安装了:

`# apt-get install wget`

要在 Red Hat 衍生产品上安装 Wget，请使用以下命令:

`# yum install wget`

## 长支持和简写支持

事实上，Wget 遵循“GNU getopt”标准，这意味着 Wget 将同时处理其命令行选项。例如，注意`--no-clobber`选项与`-nc`选项相同。让我们来看看如何使用 Wget 通过该选项下载一些东西——如果下载被中断，您将要求它使用幽默地命名的`no-clobber`选项来避免覆盖先前下载的文件:

`# wget –no-clobber`[`http://www.chrisbinnie.tld`T3】](http://www.chrisbinnie.tld/)

可以使用以下命令从命令行运行相同的命令:

`# wget -nc`[`http://www.chrisbinnie.tld`T3】](http://www.chrisbinnie.tld/)

我提到这一点是因为这两种格式选项各有其位置。例如，当你开始处理这个包时，你可能很快就会缩写所有的内容，最后可能会在你写的脚本中插入速记。

然而，当你在一个包的选项中找到自己的路时，最初手写出来会容易得多。此外，如果您正在准备一个其他人将来会使用和管理的脚本，那么查找每一个命令行参数来解释一个复杂的命令会花费相当多的时间。手写可能也更适合这种情况。现在，为了清楚起见，我将尝试使用这两种格式来解释每个命令行选项，并让您选择其中一种。

您可能已经从我提到的功能中猜到了，Wget 还支持“重新获取”或“恢复”下载。这与提供这种功能的服务器有天壤之别。您只需从已经成功下载的文件中挑选一部分，毫不浪费时间地直接回到手头的任务。而且，对于 10GB 的文件，这通常是必要的，而不是精确的。

## 记录错误

但是，如果神奇的 Wget 在运行过程中遇到错误怎么办？如果它从脚本或命令行中运行，您可以在单独的日志文件中收集错误。只需要一个非常简单的解决方案，这次使用`-o`开关:

`# wget --output-file logfile_for_errors`[`http://www.chris.tld/chrisbinnie.tar.gz`T3】](http://www.chris.tld/chrisbinnie.tar.gz)

回到命令行。如果您没有从脚本中运行 Wget，那么您可以在启动 Wget 后直接将其置于后台模式。在这种情况下，如果没有声明`-o`日志文件，那么 Wget 将在这种情况下创建`wget-log`来捕获输出。

另一个日志选项是将其附加到已经存在的日志文件中。如果您正在通宵执行一些自动化脚本，例如使用 Cron 作业，那么一个通用 Wget 日志文件可能位于`/var/log`目录中，并由一个自动化日志轮换工具(如`logrotate`)进行轮换。

您可以使用`-a`选项实现此功能，如下所示:

`# wget –append-output=/var/log/logfile_for_errors`[`http://www.chris.tld/chrisbinnie.tar.gz`T3】](http://www.chris.tld/chrisbinnie.tar.gz)

关于错误和发现运行 Wget 后发生了什么，您也可以选择`-d`来启用调试。这只是手写的`--debug`。

有一点需要注意的是，一些系统管理员没有将该选项编译到 Wget 中，从而禁用了该功能。如果它对你不起作用，那很可能就是原因。

您可以启用`-q`来抑制输出，而不是一直将 Wget 输出重定向到`/dev/null`。您可能已经猜到，这可以通过以下方式实现:

`# wget --quiet`[`ftp://ftp.chrisbinnie.tld/largefile.tar.gz`T3】](ftp://ftp.chrisbinnie.tld/largefile.tar.gz)

还有比这更多的选择。在不启用调试的情况下，您总是可以使用 Linux 标准`-v`来启用`--verbose`输出选项，这在大多数情况下会有所帮助。如果事情不像你希望的那样顺利，它至少有助于诊断简单的问题。

使用您已经习惯的经过充分考虑的方法，永远可靠的 Wget 甚至迎合了代表`--no-verbose`的`-nv`选项。这允许您禁用详细模式，但不关闭所有噪音。因此，您仍然会看到一些简单的事务信息和任何有用的生成的错误。

## 自动化任务

之前的功能并不是唯一让 Wget 如此受欢迎的功能；它还拥有一些选项，我想这些选项主要是为自动化设计的。

例如，`-i`(“输入文件”)让你不只是抓取一个 URL，而是尽可能多的抓取它。您可以使用此方法指定一个顶部充满 URL 的文件，如下所示:

`# wget --input-file=loads_of_links`

你也可以选择过去被称为“递归下载”的方式。换句话说，当 Wget 发现一个到另一个页面的链接，从你指向的资源链接，然后 Wget 将整理这些图像、文件和网页的本地副本。真的是指附在那一页上的任何东西。可选参数`--recursive`命令与`-r`一样简单，缩写如下:

`# wget -r`[`http://www.chrisbinnie.tld/inner-directory/files`T3】](http://www.chrisbinnie.tld/inner-directory/files)

它将同时接收 HTML 和 XHTML。此外，Wget 考虑到了目录结构，这意味着以一种非常方便的方式，您可以像在网上冲浪一样点击内容的本地副本。

而且，有点像创建你自己的网页内容——收集蜘蛛，Wget 甚至会注意在网站根目录中找到的标准`robots.txt`文件命令。robots 文件可以指示任何自动化工具，至少会注意什么是公共内容，允许下载，什么应该被搁置。还有一大堆其他的说明，那是改天再说的。

当您使用`-i`选项从脚本中获取内容(通常称为“抓取”)并将其保存在本地时，使用`-B`选项在资源(以及任何后续资源)的开头添加一个 URL 也很有用。这允许您跟踪资源来自哪个网站。为此，请将顶级 URL 添加到目录结构中。

这可以通过以下方式实现:

`--base=`[`http://www.chrisbinnie.tld`T3】](http://www.chrisbinnie.tld/)

我在脚本中多次使用过的一个特性是`-O`选项(不要与小写的`-o`标志相混淆)，其中的内容在下载后会被`grep`、`awk`和`sed`仔细检查。

使用该选项，您可以将下载的内容拖放到单个文件中，任何其他下载的内容都将被附加到该文件中。这不是模仿一个目录中有许多小文件的下载网页。

## 通过重试完成请求

在线自动化任何任务的一个关键考虑是考虑到不可避免地会有问题，这将导致不同程度的痛苦。您应该预料到连接问题、中断、停机以及垃圾邮件、机器人和恶意软件等普遍的互联网问题。

您可以使用`-t`开关让 Wget 忠实地尝试完成一个请求；或者，在手写中，您可以使用`--tries=10`添加 Wget 放弃之前资源将进行的十次重试的次数。这个可配置的参数确实有助于连接不良或目标不可靠的情况。

考虑这样一个场景，您希望从 HTML 中获取链接并下载内容，这些内容保存在本地的一个文件中。`-F`或`--force-html`开关将允许这样做。与命令开关`--base`一起使用，从你的目标中拾取相对的，而不是绝对的链接。

在谈到连接性问题后，考虑一下这个问题。如果你因为刚刚看到时间而不得不突然将笔记本电脑置于暂停模式，该怎么办？`-c`或`--continue`的争论让你稍后继续，让可靠的 Wget 完全不知道你在 2:30 的牙科预约上迟到了。

您甚至不必添加`--continue`开关，因为这是默认选项。但是，如果您想恢复当前实例之前的下载运行，则需要它。

然而，一个超级快速的警告。在 Wget 的更高版本中，如果您要求它在已经包含一些内容的文件上启动“resume ”,而您连接的远程服务器不支持 resume 功能，它将错误地覆盖现有内容！

要保存现有内容，您应该在开始下载之前更改文件名，然后再尝试合并文件。

## 显示进度

另一个不错的选择是进度条。有多少次程序只是坐着，看起来很忙，即使它们并不忙，而你却似乎永远在等待？

改变默认的进度条格式没有捷径，但不用担心，因为添加`--progress=dot`会给你一个点计数器，而用`bar`而不是`dot`会用永远忠实的 ASCII 艺术画出一个完美可信的进度条

说到显示选项，增加`-Q`怎么样？以下将让您以兆字节或千字节(在这种情况下是后者)显示下载详细信息:

`# wget --quote=k`[`https://chris.binnie.tld/anotherfile.zip`T3】](https://chris.binnie.tld/anotherfile.zip)

## DNS 注意事项

我在过去遇到的关于底层基础设施问题的一个问题，不是我想指出的 Wget，已经被确定为 DNS 缓存问题。

过去，相对较少的面向国内市场的互联网服务提供商不太关心代理网络内容以节省带宽。他们觉得没有必要缓存流行的内容(所以内容下载一次，而不是多次，减少了 ISP 的带宽使用，从而降低了成本)。

这同样适用于域名系统查找(尽管 DNS 查找产生极少量的流量，但仍然存在服务器负载问题)。在我们生活的这个现代互联网时代，为数百万用户提供接入的 ISP 往往将到期时间设置得更高(一两天，而不是过去足够的一两个小时)。

令人惊讶的是，使用 Wget，您可以禁用本地化缓存依赖(不幸的是，我应该指出这不会影响底层基础设施),至少可以防止精心设计的 Wget 不时导致相同的缓存问题。事实上，Wget 是一个足够慷慨的应用程序，可以首先提供缓存，这一点并不常见。缓存的查找只保存在内存中，通过重新运行 Wget，这将意味着在启用该选项的情况下向名称服务器发送新的查询。

## 使用身份验证

如果您因为网站受密码保护而无法访问该网站，该怎么办？如您所料，Wget 也考虑了这种常见场景。然而，还有不止一个选项供你探索。

最明显的方法是在命令行中输入登录名和密码，如下所示:

`# wget --user=chris --password=binnie`[`http://assets.chrisbinnie.tld/logo.jpg`T3】](http://assets.chrisbinnie.tld/logo.jpg)

但是坚持一会儿。如果您了解 Bash 的历史，不要介意别人的窥探，从安全角度来看，这种方法并不理想。

相反，Wget 让您将这种方法提供给命令行，在命令行中，您必须在提示符下键入密码。

`# wget --user=chris --ask-password`[`http://assets.chrisbinnie.tld/logo.jpg`T3】](http://assets.chrisbinnie.tld/logo.jpg)

对于其他主要协议，您还可以分别将`--ftp-user`、`--ftp-password`、`--http-user`和`--http-password`丢入环中。

## 否认和制作饼干

如果您想到了一个让您头疼的明显的 HTTP 副作用，那么请放心，Wget 已经涵盖了它。

您可以拒绝所有 cookies(收集服务器端统计信息和会话数据),或者按照您想要的任何方式制作它们。

有时有必要精心制作非常特殊的 cookies，以便 Wget 可以正确地导航网站，模仿用户的所有意图和目的。您可以通过`--load-cookies`选项实现这一效果。只需添加一个文件名作为该选项的后缀，然后您就可以通过使用以前生成的 cookie 数据有效地欺骗登录到一个站点。这意味着您可以轻松地在该网站上移动。

## 创建目录结构

在你的下载和保存选项中有`-nd`选项，也称为`--no-` `directories`。此外，还有`–x`，这是它的反义词(允许您通过逗号分隔的列表排除目录)。这仍然允许您创建一个目录结构，并拉入所有跟随的链接。

这对于分割不同的数据捕获可能是有用的，即使它只是一个网站的一个页面，从脚本运行的命令中获取。

然后，我们不要忘记在保存的目录的开头添加基本 URL。`-nH`或`--no-host-directories`标志可以让您避免完全包含基本 URL。

## 精确

对于你们当中的学究来说，在这些保存的目录名的开头删除协议的内容怎么样？嗯，`--protocol-directories`应该只将目录保存为`hostname`，而不在前面加上`http`、`https`或`ftp`。

我过去遇到过一个问题，是关于电子邮件客户端如何处理由脚本创建的内容生成的字符集。您可以使用优秀的 Wget 随心所欲地制作标题。本质上，你最终创建了一个假的网页浏览器标题。这可以通过以下方式实现:

`# wget --header=’Accept-Charset: iso-2057’ --header=’Accept-Language: en-gb’`[`http://www.chrisbinnie.tld`T3】](http://www.chrisbinnie.tld/)

任何看到浏览器抱怨达到最大重定向数的人都会看穿这个选项:`--max-redirect=`。如果在末尾添加一个数字，一旦超过最大限制，web 服务器重定向将生成一个错误。默认值为 20 英镑。

## 安全

使用 HTTP referrer 字段可以实现一种非常基本的安全形式。它可能会如何使用？一个常见的情况可能是，网站管理员不想启用身份验证来允许对资源的访问，而是希望添加简单的安全级别，这样人们就不能直接链接到该最终资源。

一种方法是强制执行请求来自哪个页面。换句话说，你只能访问`PageTwo.html`，例如，通过点击`PageOne.html`的链接。这种松散的安全形式可以很容易地被绕过(恶意或其他方式),方法是包含这样一个独特的引用:

`# wget --referrer=PageOne.html PageTwo.html`

### 入账日期

继续向前，您可以沿着 URL 传递更复杂的数据，就像直接使用动态脚本语言一样，比如 PHP 或 ASP。

添加的`--post-data=`参数允许您包含一串数据，如一个或两个变量名及其值集，如下所示:

`http://www.chrisbinnie.tld/page.php?firstname=chris&secondname=binnie`。

然而，有时候你需要传递更多的数据，因为互联网的 URL 越来越长。您可以简单地用`--post-file=`后跟一个充满 HTTP `POST`数据的文件名来做到这一点。我觉得很方便。

### 密码

面对扩展加密，可以添加`--secure-protocol=name_of_protocol`参数。您可以使用 TLS v1(SSL 的继任者)、SSLv2 或更现代的 SSLv3。

您也可以改为请求“auto ”,这使得捆绑的 SSL 库能够做出明智的选择。我过去遇到的一个令人头痛的问题是过期的 SSL 证书(来自管理不善的服务器)或自签名证书偶尔会导致异常响应。简单的修复`--no-check-certificate`对脚本来说是极好的，并且挽救了局面。

### 首字母缩略词

最大的文件往往位于 FTP 服务器上，因此 Wget 处理正确下载它们所需的访问权限非常重要。如果我告诉你它漂亮地跨越了这个障碍，你肯定不会从座位上掉下来。

除了`--ftp-user=chris`和`--ftp-password=binnie`参数之外，智能 Wget 还会默认为`-wget@`，这将满足匿名 FTP 登录(匿名登录如果你之前没遇到过的话一般是`login: email-address`和`password: anonymous`)。

你应该知道你也可以在 URL 中加入密码。然而，通过在 URL 中嵌入密码，以及简单的用户和密码参数，系统上任何运行`ps`命令来查看进程表的用户都会遗憾地看到您的密码，但就像变魔术一样。几乎不安全，我想你会同意。

您将很快看到预先命名的配置文件:`.wgetrc`或者另一个名字是`.netrc`。

不过，在你到达那里之前，先给你提个醒。如果密码不是最重要的，那么您不应该在我刚才提到的两个配置文件中的任何一个中以纯文本的形式保存它们。

文件下载开始后，您可以随时删除它们。我相信通过一些创造性的脚本，您可以解密密码，将密码放入 Wget 配置文件，然后在传输开始后忠实地删除它。

如果你过去遇到过防火墙遇到 FTP 的问题，我肯定你听说过被动模式。您可以通过将`--no-passive-ftp`选项添加到 FTP 来规避这些令人头疼的问题，而防火墙通常更喜欢启用被动模式。

如果您的 Wget 客户端遇到文件系统符号链接(对于不了解的人来说，符号链接有点像 Windows 上的快捷方式)，那么有时它不会与实际文件一起出现，而是看起来像一个符号链接。然而，正如你现在所期望的，强大的 Wget 让你用`--retr-symlinks`决定这些快捷方式的命运。

符号链接的目录对 Wget 来说太痛苦了，所以要小心以免引起眼睛疲劳。

我已经提到了收获子目录的递归`-r`选项。一个警告是，有时通过跟踪链接，你会不经意地得到一个又一个千兆字节的数据。如果做不到这一点，您最终可能会得到成千上万的小文件(由图像、文本文件、样式表和 HTML 组成),这给管理带来了麻烦。您可以使用`-l`或`--level=5`明确指定在子目录中搜索的深度；在这种情况下设置为五个级别。

### 委托书

另一种情况可能如下。您已经建立了一个代理服务器，位于 web 和您的数百个用户之间。您已经对它进行了彻底的测试，它运行良好，但是您希望在数百个用户开始使用代理服务器并因负载突然增加而对其造成压力之前，用流行的网页填充缓存。

在周一早上到来之前，你有整整一个周末的时间，所以你决定开始复制并粘贴一长串几乎肯定会被你的用户看到的热门网址(同时禁止一些不合适的网址)。您知道可以用各种方法填充文件并将其输入 Wget，但是如果您只是想访问 web 站点来填充代理的缓存，那么您肯定不希望将下载到访问它们的机器上的所有数据都保存在代理中。

Wget 使用`--delete-after`标志非常简单地处理了这个问题。它以一种真正正常的方式访问站点(如果需要的话，就像一个假的 web 浏览器一样小心地设置，传递正确的用户代理参数)。一旦你设置好了，你只需要清除你收集的所有数据。

不用担心——这不会因为你访问的 FTP 服务器上的文件被意外删除而招致批评。这很安全。

### 反映

如果您想在本地保存自己网站的完整备份，可以使用 Wget 通过每天的 Cron 作业抓取内容，并使用`-m`或`–` `mirror`选项。

通过采用这种方法以及将目录深度设置为“无限”并启用时间戳，您可以立即启用递归下载。根据 Wget 的手册页，镜像特性一下子增加了以下选项:T0、T1、T2 和 T3。

## 下载资产

有时候当你抓取网页时，结果可能会令人失望。如前所述，您实际上是在创建一个假的浏览器，Wget 不可能达到一个全功能的浏览器，尤其是它的文件大小很小。

要获取组成网页的所有元素，您可能需要启用`-p`或`--page-requisites` "=来解决这个问题。

在合理的范围内，这非常有意义，因为今天的网络现实是一个样式表和高度图形化的内容。

另一个值得一提的有用特性意味着(因为 Windows 通常不区分文件名的大小写，但类 Unix 操作系统区分大小写)您可以选择不去注意带有`--ignore-case`的有点有挑战性的、好的、恼人的文件名。

## 持久配置

正如我提到的，您可以在运行时或者在本地用户的配置文件中指定这些参数。这个例子使用了`wgetrc`而不是`.netrc`。

例如，你可能总是想要千字节。而且，您知道您的远程文件目标在一个不可靠的服务器上，所以您总是需要大量的重试才能得到正确的结果。这里有一个样本`.wgetrc`，在我的例子中，它将存在于`/home/chrisbinnie/.wgetrc`中。

`Sample .wgetrc`

`============`

`quota = k # Not MegaBytes`

`tries = 50 # Lots of retries`

`reclevel = 2 # Directory levels to delve down into`

`use_proxy = on # Enable the proxy`

`https_proxy =`[`http://proxy.chris.tld:65530/`](http://proxy.chris.tld:65530/)T2】

`http_proxy =`[`http://proxy.chris.tld:65531/`T3】](http://proxy.chris.tld:65531/)

`ftp_proxy =`[`http://proxy.chris.tld:65532/`T3】](http://proxy.chris.tld:65532/)

`wait = 2 # Wait two seconds before hammering the bandwidth again`

`httpsonly = off # In recursive mode don’t follow links which aren’t HTTPS for security`

`secureprotocol = auto # Finely tweak your preferred encryption protocol version`

很明显，这个文件中可以包含大量的其他选项。我会让你用你的想象力和诡计来挑选和你的精确需求相关的。反复试验会有很大帮助。我喜欢小型 Linux 实用程序的一点是不必等待完整的迭代。如果你想中途中断一次发射，你只需按 Ctrl+C。

## 犯罪活动笔记

现在谈一个更严肃的话题。

我曾经花了大量的时间来保护一个面向公众的 web 服务器，由于我感到沮丧的原因，我发现很难避免使用 Wget 的替代品 Curl。

为什么我花了这么多精力来避免在我的服务器上安装这样一个神奇的工具？原因很简单，如果我的服务器出现小漏洞，我知道我可能会有大麻烦。

我最关心的是系统上的 Apache web 服务器用户(`www-data`、`httpd`，甚至 Apache 是过去使用的 Apache 用户名)被利用。我知道 Wget 和 Curl 可以用来将恶意数据下载到服务器上，然后几乎可以肯定地执行这些数据来控制我的服务器。最糟糕的情况。

你可能会问为什么我对这种威胁如此确信。简单的回答是，我过去亲眼目睹了这种类型的攻击。如果您熟悉 Apache，那么您会知道，作为标准，在不调整配置设置的情况下，对 web 服务器的任何点击都会记录到文件`/var/log/apache2/access.log`中。这个文件名稍有变化，但只是细微的变化，这取决于发行版。

虽然对服务器的所有点击都在那里结束，但是任何错误或信息性消息都被写入到`/var/log/apache2/error_log`文件中。显然，将这两个元素分成不同的文件会使管理更加容易。

当我被要求帮助恢复一个我正在帮助恢复的受损服务器时，令我非常害怕的是，在 Apache 的错误日志中出现了以 Wget 开头的一行。

这很容易发现，因为在每一行的开头都特别缺少时间戳。在 Wget 输出的少数几个条目周围的行中也有一些无意义的字符。

它显示这台服务器已经远程连接到一个看起来不可靠的网站，附加的文件名似乎是几个随机字符来伪装它。您瞧，这个服务器已经通过一个流行的 PHP 论坛应用程序受到了威胁，自动化 Apache 用户无意中在错误日志中记录了它的活动。粗心的攻击者没有清理日志文件，尽管获得了超级用户权限(因此他们有完全的权限来整理和伪装他们的入侵路线)。

经过一番挖掘，我很快发现了一个非常讨厌的 rootkit，它被配置为在每晚午夜将任何用户的登录和密码发送回外国。此外，为了防止该服务器的系统管理员变得明智并试图清除感染，有一个辅助 SSH 守护程序在一个隐藏的端口上运行，该端口在`netstat`中没有显示(但在`lsof`中显示了)，这要感谢被感染版本覆盖的`netstat`二进制文件。

不要误解我——这次攻击之所以成功，是因为 PHP 应用程序中的一个错误，而不是 Wget 的错。然而，这是对安全新手的警告。

您在服务器上留下的工具(如编译器和无害的下载程序)越多，服务器就越容易控制，即使发生了相对较小的违规事件。而且，这很容易意味着没有额外工作和从头开始花整整三天重建服务器之间的区别。如果你像我一样，你不喜欢为自己做额外的工作。

## 摘要

最后一句警告:不要试图非法抓取内容。正如我已经演示过的，Wget 的过程非常简单，Curl 就是这样，毫不费力地复制面向公众的内容并保存到本地驱动器。然而，不言而喻，你应该意识到潜在的版权问题。冒着重复系统管理员圈子里常说的一句话的风险:“拥有强大的力量……”。

这一章的目的是激发人们对超级英雄 Wget 的兴趣；有时被认为是理所当然的特殊工具。将 Wget 很好地用于诊断目的和各种各样的脚本，它是一个非常棒的武器，应该包含在任何系统管理员的武器库中。通过一些创造性的思考，我相信您会像我一样喜欢尝试 Wget。