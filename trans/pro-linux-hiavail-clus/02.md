# 2.配置存储

几乎所有的集群都以某种方式使用共享存储。本章介绍如何将集群连接到共享存储。除了连接到共享存储，您还将学习如何在自己的环境中建立 iSCSI 存储区域网络(SAN ),这个主题将在第 10 章中进一步探讨。您还将了解网络连接存储(NAS)和 SAN 之间的区别，以及何时使用哪种存储。本章包括以下主题:

*   为什么大多数集群需要共享存储
*   NAS 还是 SAN？
*   iSCSI 还是光纤通道？
*   配置 LIO iSCSI 目标
*   连接到 iSCSI SAN
*   设置多路径

## 为什么大多数集群需要共享存储

在高可用性集群中，您将确保重要资源尽可能可用。这意味着，在某个时间，您的资源可能在一个节点上运行，而在另一个时间，资源可能在另一个节点上运行。然而，在另一个节点上，资源将需要访问完全相同的文件。这就是为什么共享存储可能会派上用场。

如果您的资源只处理静态文件，您可以不使用共享存储。如果很少修改文件，您可以手动复制文件，或者使用 rsync 等解决方案将文件同步到集群中的其他节点。但是，如果数据是动态的，并且变化频繁，您将需要共享存储。

通常，一个资源使用三种不同的文件。首先，有构成资源提供的程序或服务的二进制文件。最好在每台主机上本地安装这些二进制文件。这将确保每台主机在其更新过程中都将更新所需的二进制文件，并且它将确保如果集群发生非常糟糕的事情，并且您被迫独立运行所有内容，主机仍然可以运行应用程序。

通常使用的第二部分数据是配置文件。即使许多应用程序默认将配置文件存储在本地`/etc`目录中，大多数应用程序也可以选择将配置文件存储在其他地方。将这些配置文件放在共享存储上通常是个好主意。这确保了您的集群应用程序总是能够访问相同的配置。理论上，在主机之间手动同步配置文件也是可行的，但是在现实生活中，总是会出错，并且您可能会以同一应用程序的两个不同版本而告终。因此，请确保将配置文件放在共享存储上，并将您的应用程序配置为从共享存储而不是本地访问文件。

应用程序通常使用的第三种也是最重要的文件类型是数据文件。这些通常是公司的宝贵资产，而且它们必须在任何时候都可以从所有节点获得。这就是为什么集群中的节点被配置为访问 s an 磁盘，而数据文件存储在 SAN 磁盘上。这确保了所有主机在任何时候都可以从 SAN 访问文件。SAN 本身以冗余方式设置，以确保文件受到高度保护，不会因为配置不良而导致服务中断。该设置的概述见图 [2-1](#Fig1) 。

![A978-1-4842-0079-7_2_Fig1_HTML.jpg](A978-1-4842-0079-7_2_Fig1_HTML.jpg)

图 2-1。

Cluster application file access overview

## NAS 还是 SAN？

为共享存储选择合适的解决方案时，您必须在网络连接存储(NAS)和存储区域网络(SAN)之间做出选择。我们来讨论一下这两者的一些区别和优势。

### 噪声控制协会

网络连接存储(NAS)基本上是一种网络共享，可以由网络上的任何服务器提供。在 Linux 集群中，NAS 通常是通过网络文件系统(NFS)共享来提供的，但是通用互联网文件系统(CIFS)也是提供 NAS 功能的有效选项。NAS 的优点是设置简单。不过，还有一些其他的考虑。

通常，NAS 服务由网络中的服务器提供。现在，在设置集群环境时，最重要的是避免网络中出现单点故障。因此，如果您计划在集群环境中设置一个 NFS 服务器来提供共享存储，那么您也必须将它集群化，以确保在主 NFS 服务器中断时共享存储仍然可用。因此，您必须将 NFS 或 CIFS 服务器集群化，并确保无论服务本身在哪里运行，它都能访问相同的文件。使用 NFS 或 CIFS 的高可用性 NAS 服务器通常应用在高可用性集群环境中。

在高可用性环境中使用 NAS 解决方案的一个常见原因是，NAS 提供并发文件系统访问，而 SAN 不会，除非它在客户端设置了 OCFS2 或 GFS2。

### 疗养院

存储区域网络(SAN)经过定制，可提供尽可能最佳的冗余以及访问存储的性能(图 [2-2](#Fig2) )。它通常由磁盘阵列组成。要访问这些磁盘，需要使用专用的网络基础设施。

![A978-1-4842-0079-7_2_Fig2_HTML.jpg](A978-1-4842-0079-7_2_Fig2_HTML.jpg)

图 2-2。

SAN overview

SAN 文件服务器中的磁盘通常是使用 RAID 设置的。通常，不同的 RAID 阵列被配置成确保 SAN 能够经受住几个磁盘同时崩溃。在这些 RAID 阵列之上，创建逻辑单元号(LUN)。可以授权群集中的节点访问特定的 LUN，这些 LUN 对它们来说将显示为新的本地磁盘。

为了访问 SAN 文件服务器，通常使用冗余网络基础架构。在此基础架构中，大多数项目都是双重的，这意味着节点有两个 SAN 接口连接到两个 SAN 交换机，而这两个交换机又连接到 SAN 上的两个不同的控制器。所有这些都是为了确保如果某个东西失败了，最终用户不会注意到任何东西。

## iSCSI 还是光纤通道？

一旦您决定使用存储区域网络(SAN ),下一个问题就出现了:它将是光纤通道还是 iSCSI？市场上出现的第一批 San 是光纤通道 San。这些文件服务器针对最佳性能进行了优化，并且还使用了专用的 SAN 网络基础架构。这是因为在第一个 SAN 解决方案出现的时候，100 Mbit/s 是局域网上最快的速度，与本地 SCSI 总线上的吞吐量相比，这太慢了。此外，当时的网络大部分时间都在使用集线器，这意味着网络流量的处理效率相当低。

然而，时代变了，局域网变得越来越快。千兆位是当前网络的最低标准，几乎所有的集线器都已被交换机取代。在这些改进网络的背景下，一个新的标准诞生了:iSCSI。iSCSI 背后的想法很简单:在本地磁盘基础设施上生成和发送的 SCSI 数据包被封装在 IP 报头中，以寻址 SAN。

光纤通道 SAN 享有比 iSCSI 更可靠、更快的声誉。不过，这不一定是真的。市场上提供了一些高端 iSCSI 解决方案，如果使用专用的 iSCSI 网络，其中流量被优化以处理存储，iSCSI 可以像光纤通道 SAN 一样快速和可靠。iSCSI 确实有光纤通道 SAN 所不具备的优势，而且这是创建 iSCSI SAN 解决方案的相对简单的方法。例如，在本章中，您将学习如何自己设置 iSCSI SAN。

实施光纤通道技术而无需购买昂贵的光纤通道硬件的另一种替代方案是使用以太网光纤通道(FCoE)。此解决方案允许光纤通道使用 10 千兆以太网(或更快)，同时保留光纤通道协议。FCoE 解决方案可以在主要的 Linux 发行版中获得。

### 了解 iSCSI

在 iSCSI 配置中，您要处理两个不同的部分:iSCSI 目标和 iSCSI 发起者(图 [2-3](#Fig3) )。iSCSI 的目标是存储区域网络(SAN)。它运行 SAN 的 TCP 端口 3260 上提供的特定软件，并提供对 SAN 上提供的逻辑单元号(LUN)的访问。iSCSI 启动器是在群集中的节点上运行并连接到 iSCSI 目标的软件。

![A978-1-4842-0079-7_2_Fig3_HTML.jpg](A978-1-4842-0079-7_2_Fig3_HTML.jpg)

图 2-3。

iSCSI overview

要连接到 iSCSI 目标，需要使用专用的 SAN 网络。它通常是一个常规的以太网，但以特定的方式配置。首先，网络是多余的。这意味着群集节点上的两个不同的网络接口连接到两个不同的交换机，这两个交换机又连接到 SAN 上的两个不同的控制器，每个控制器也可以由两个不同的网络接口访问。这意味着至少有四条不同的路径可以访问 SAN 上共享的 LUN。这导致每个 LUN 都有被群集节点看到四次的风险。您将在本章后面的多路径章节中了解更多信息。

在 SAN 网络上，也可以应用一些特定的优化。优化从集群节点开始，管理员可以选择不是普通的网卡，而是 iSCSI 主机总线适配器(HBA)。这些智能网卡是为以最佳方式处理 iSCSI 流量而生产的。他们将以太网级别的最大数据包大小设置为 9000 字节的 MTU，以确保尽可能快地处理流量，他们通常使用 iSCSI 卸载引擎来更高效地处理 iSCSI 流量。然而，iSCSI HBA 已经变得不那么受欢迎，并且有被快速网络接口卡(NIC)所取代的趋势。

## 配置 LIO iSCSI 目标

市场上有许多不同的供应商提供 iSCSI 解决方案，但是您也可以在 Linux 上设置 iSCSI。Linux-IO (LIO)目标是最新发行版中 Linux 最常见的 iSCSI 目标(图 [2-4](#Fig4) )。你会在所有最近发行的版本中找到它。例如，在 SUSE Linux Enterprise Server 12 上，您可以很容易地从 YaST 管理实用程序中设置它。在其他发行版中，您可能会找到`targetcli`实用程序来配置 iSCSI 目标。当然，在设置单个 iSCSI 目标时，您必须意识到这可能是一个单点故障。在本章的后面，你将学习如何以冗余的方式设置 iSCSI 目标。

![A978-1-4842-0079-7_2_Fig4_HTML.jpg](A978-1-4842-0079-7_2_Fig4_HTML.jpg)

图 2-4。

Setting up the LIO Target from SUSE YaST

设置目标时，必须指定所需的组件。其中包括以下内容:

*   存储设备:这是您将要创建的存储设备。如果您使用 Linux 作为目标，使用 LVM 逻辑卷作为底层存储设备是有意义的，因为它们非常灵活。但是您也可以选择其他存储设备，比如分区、完整硬盘或稀疏文件。
*   LUN ID:与 iSCSI 目标共享的每个存储设备都作为一个 LUN 共享，并且每个 LUN 都需要一个唯一的 ID。LUN ID 类似于分区 ID；唯一的要求是它必须是唯一的。为此目的选择后续的数字 LUN IDs 没有错。
*   目标 ID:如果您想将目标授权给特定的节点，创建不同的目标是有意义的，其中每个目标都有自己的目标 ID，也称为 Internet 限定名(IQN)。从 iSCSI 客户端，您需要目标 ID 来进行连接，因此请确保目标 ID 有意义，并且便于您识别特定的目标。
*   标识符:标识符帮助您进一步识别特定的 iSCSI 目标。
*   端口号:这是目标将监听的 TCP 端口。默认情况下，端口 3260 用于此目的。

以下程序演示了如何使用`targetcli`命令行实用程序来设置 iSCSI 目标:

Start the iSCSI target service, using `systemctl start target.service`.   Make sure that you have some disk device to share. In this example, you’ll read how to share the logical volume `/dev/vgdisk/lv1`. If you don’t have a disk device, make one (or use a file for demo purposes).   The `targetcli` command works on different backstores. When creating an iSCSI disk, you must specify which type of backstore to use. Type `targetcli` to start the `targetcli` and type `backstores` to get an overview of available backstores.  

`/>ls`

`o- / .....................................................................``[``...`T3】

`o-` `backstores` `........................................................` `[` `...`

`| o-` `block` `..............................................` `[` `Storage Objects: 0`

`| o-` `fileio` `.............................................` `[` `Storage Objects: 0`

`| o-` `pscsi` `..............................................` `[` `Storage Objects: 0`

`| o-` `ramdisk` `............................................` `[` `Storage Objects: 0`

`o-` `iscsi` `......................................................` `[` `Targets: 0`

`o-` `loopback` `...................................................` `[` `Targets: 0`

Now let’s add the LVM logical volume, using the following command:  

`/backstores/block create lun0 /dev/vgdisk/lv1`

如果没有可用的物理存储设备，出于测试目的，可以使用以下命令为稀疏磁盘文件创建 iSCSI 目标:

`/backstores/fileio create lun1 /opt/disk1.img 100M`

At this point, if you type `ls` again, you’ll see the LUN you’ve just created.  

`/>ls`

`o- / .....................................................................``[``...`T3】

`o-` `backstores` `..........................................................` `[` `...`

`| o-` `block` `..............................................` `[` `Storage Objects: 1`

`| | o-` `lun0` `..............` `[` `/dev/vgdisk/lv1 (508.0MiB) write-thru deactivated`

`| o-` `fileio` `.............................................` `[` `Storage Objects: 0`

`| o-` `pscsi` `..............................................` `[` `Storage Objects: 0`

`| o-` `ramdisk` `............................................` `[` `Storage Objects: 0`

`o-` `iscsi` `........................................................` `[` `Targets: 0`

`o-` `loopback` `.....................................................` `[` `Targets: 0`

Now you need to define the target itself.  

`/> /iscsi create`

`Created target iqn.2003-01.org.linux-iscsi.localhost.x8664:sn.9d07119d8a12.`

`Created TPG 1.`

Type `cd`. It gives an interface that shows all currently existing objects, from which you can select the object you want to use with the arrow keys.  

`o- / .....................................................................[...]`

`o- backstores ..........................................................[...]`

`| o- block ..............................................[Storage Objects: 1]`

`| | o- lun0 ..............[/dev/vgdisk/lv1 (508.0MiB) write-thru deactivated]`

`| o- fileio .............................................[Storage Objects: 0]`

`| o- pscsi ..............................................[Storage Objects: 0]`

`| o- ramdisk ............................................[Storage Objects: 0]`

`o- iscsi ........................................................[Targets: 1]`

`| o- iqn.2003-01.org.linux-iscsi.localhost.x8664:sn.9d07119d8a12 ...[TPGs: 1]`

`|   o- tpg1 ...........................................[no-gen-acls, no-auth]`

`|     o- acls ......................................................[ACLs: 0]`

`|     o- luns ......................................................[LUNs: 0]`

`|     o- portals ................................................[Portals: 0]`

`o- loopback .....................................................[Targets: 0]`

使用箭头键选择您刚刚创建的`tpg1`对象。

Now, type `portals/ create` to create a portal with default settings.  

`/iscsi/iqn.20...119d8a12/tpg1> portals/ create`

`Using default IP port 3260`

`Binding to INADDR_ANY (0.0.0.0)`

`Created network portal 0.0.0.0:3260.`

Now, you can actually assign the LUN to the portal.  

`/iscsi/iqn.20...119d8a12/tpg1> luns/ create /backstores/block/lun0`

`Created LUN 0.`

And if you want to, limit access to the LUN for a specific iSCSI initiator, using the IQN of that iSCSI initiator (typically, you can get the IQN from the `/etc/iscsi/initiatorname` file).  

`acls/ create iqn.2014-03.com.example:123456789`

Use `cd /` and `ls` to view the current settings.  

`/>ls`

`o- / .....................................................................``[``...`T3】

`o-` `backstores` `..........................................................` `[` `...`

`| o-` `block` `..............................................` `[` `Storage Objects: 1`

`| | o-` `lun0` `................` `[` `/dev/vgdisk/lv1 (508.0MiB) write-thru activated`

`| o-` `fileio` `.............................................` `[` `Storage Objects: 0`

`| o-` `pscsi` `..............................................` `[` `Storage Objects: 0`

`| o-` `ramdisk` `............................................` `[` `Storage Objects: 0`

`o-` `iscsi` `........................................................` `[` `Targets: 1`

`| o-``iqn.2003-01.org.linux-iscsi.localhost.x8664:sn.9d07119d8a12 ... [``TPGs: 1`T3】

`|   o-` `tpg1` `...........................................` `[` `no-gen-acls, no-auth`

`|     o-` `acls` `......................................................` `[` `ACLs: 0`

`|     o-` `luns` `......................................................` `[` `LUNs: 1`

`|     | o-` `lun0` `...............................` `[` `block/lun0 (/dev/vgdisk/lv1)`

`|     o-` `portals` `................................................` `[` `Portals: 1`

`|       o-` `0.0.0.0:3260` `.................................................` `[` `OK`

`o-` `loopback` `.....................................................` `[` `Targets: 0`

And write the configuration and exit.  

`/>saveconfig`

`Last 10 configs saved in /etc/target/backup.`

`Configuration saved to /etc/target/saveconfig.json`

`/>exit`

`Global pref auto_save_on_exit=true`

`Last 10 configs saved in /etc/target/backup.`

`Configuration saved to /etc/target/saveconfig.json`

At this point, you have a working iSCSI target. The next section teaches you how to connect to it.  

## 连接到 iSCSI SAN

一旦存储区域网络(SAN)启动并运行，您就可以连接到它。无论您使用哪种 SAN，连接到 iSCSI SAN 的工作方式都是一样的。要连接到 SAN，您将使用`iscsiadm`命令。在您可以有效地使用它之前，这个命令需要一点解释。一些 Linux 发行版提供了一个解决方案来简化客户端配置。在 SUSE 上，该模块由 YaST 管理实用程序提供。

`iscsiadm`命令有不同的模式。每种模式在处理 iSCSI 连接的不同阶段使用。作为管理员，您通常会使用以下模式:

*   `discoverydb`或`discovery`:该模式用于查询 iSCSI 标的，了解其提供哪些标的。
*   这是你需要登录到一个特定的 iSCSI 目标的模式。
*   `session`:在此模式下，您可以获取当前会话的信息，或者与您已经连接的目标建立新的会话。
*   `iface`和`host`:这些模式允许您指定如何连接到特定目标。稍后将更详细地讨论`iface`和`host`之间的区别。

当使用 iSCSI 时，你必须知道它并不真的让你修改配置文件。要建立连接，您只需登录 iSCSI 目标。这会自动为您创建一些配置文件，这些配置文件是持久的。这意味着重新启动后，您的服务器将自动记住其最后的 iSCSI 连接。这是有意义的，因为您的服务器很可能在重新启动后必须再次连接到相同的磁盘。对于管理员来说，这意味着您必须了解配置，在某些情况下，您必须应用额外的操作来删除不再需要的 iSCSI 连接。现在，让我们看看如何创建一个具有 iSCSI 目标的新会话。

在使用`iscsiadm`命令连接到 iSCSI 目标之前，您必须确保加载了支持模块。通常，您可以通过启动 iSCSI 客户端支持脚本来实现这一点。这些脚本的名称在不同的发行版中有所不同。假设服务脚本的名字是`iscsi.service`，在 System-V 服务器上使用`systemctl start iscsi.service; systemctl enable iscsi.service (service iscsi start; chkconfig iscsi on`。为了确保加载了所有的先决条件，您可以在继续之前键入`lsmod | grep iscsi`。结果应该如下所示:

`node1:/etc/init.d #` `lsmod | grep iscsi`

`iscsi_tcp              18375    1`

`libiscsi_tcp           20820    1 iscsi_tcp`

`libiscsi               53181    2 iscsi_tcp,libiscsi_tcp`

`scsi_transport_iscsi   57581    3 iscsi_tcp,libiscsi`

`scsi_mod               231658  12 sd_mod,iscsi_tcp,libiscsi,scsi_transport_iscsi,sg,sr_mod,scsi_dh_rdac,scsi_dh_emc,scsi_dh_hp_sw,scsi_dh_alua,scsi_dh,libata`

### 步骤 1:发现模式

首先，你必须发现 iSCSI 目标能提供什么。为此，使用`iscsiadm --mode discovery --type sendtargets --portal 192.168.1.125:3260 --discover`。这个命令返回它找到的 iSCSI 目标的名称。

`iscsiadm --mode discovery --type sendtargets --portal 192.168.1.125:3260 --discover`

`192.168.1.125:3260,1 iqn.2014-03.com.example:HAcluster`

`192.168.1.125:3260,1 iqn.2014-01.com.example:kiabi`

您刚刚发出的命令不仅显示了目标的名称，还将它们置于 iSCSI 配置中，即 SUSE 上的`$ISCSI_ROOT/send_targets. ($ISCSI_ROOT`是`/etc/iscsi`，Red Hat 上的`/var/lib/iscsi`。)根据这些信息，您已经可以使用`-P`选项来打印存储在您的服务器上的关于当前模式的信息。`-P`选项后面是打印级别，类似于调试级别。所有模式都支持 0 和 1；一些模式也支持更高的打印级别。

`node1:/etc/iscsi/send_targets #` `iscsiadm --mode discoverydb -P 1`

`SENDTARGETS:`

`DiscoveryAddress: 192,3260`

`DiscoveryAddress: 192.168.178.125,3260`

`DiscoveryAddress: 192.168.1.125,3260`

`Target: iqn.2014-01.com.example:kiabi`

`Portal: 192.168.1.125:3260,1`

`Iface Name: default`

`Target: iqn.2014-03.com.example:HAcluster`

`Portal: 192.168.1.125:3260,1`

`Iface Name: default`

`iSNS:`

`No targets found.`

`STATIC:`

`No targets found.`

`FIRMWARE:`

`No targets found.`

在前面的示例中，您使用了`SENDTARGETS`发现类型。根据您的 SAN 环境，还可以使用其他发现类型。

*   iSNS 允许你设置一个 iSNS 服务器，集中注册 iSCSI 目标。
*   `firmware`是一种模式，用于能够从固件中发现 iSCSI 目标的硬件 iSCSI 适配器。
*   SLP 目前尚未实施。

### 步骤 2:节点模式

根据前一个命令的输出，您将知道目标的 IQN 名称。您将在下一个命令中需要这些，在这个命令中，您将登录到目标来实际创建连接。要登录，您将使用`node`模式。在 iSCSI 术语中，节点是指在 iSCSI 目标和特定门户之间建立的实际连接。门户是用于连接 iSCSI 目标的 IP 地址和端口号。现在，看看前面的`discoverydb`命令的输出，其中信息显示在打印级别 1 中。此命令显示，在 iSCSI 目标端口侦听的位置发现了两个不同的地址，但是这些地址中只有一个具有实际关联的目标，可以通过列出的入口到达这些目标。这直接解释了为什么下面的代码清单中的命令会失败。即使 iSCSI 端口实际上在监听提到的 IP 地址，在该 IP 地址上也没有可用的目标或门户。

`node1:/etc/iscsi/send_targets #``iscsiadm --mode node --targetname iqn.2014-01.com.example:HAcluster`T2】

`iscsiadm: No records found`

现在，让我们再次尝试 iSCSI 目标实际连接到的 IP 地址。

`node1:/etc/init.d #``iscsiadm --mode node --targetname iqn.2014-03.com.example:b36d96e3-9136-44a3-8bc9-78bd2754a137`T2】

`Logging in to [iface: default, target: iqn.2014-03.com.example:b36d96e3-9136-44a3-8bc9-78bd2754a137` `, portal: 192.168.122.1,3260] (multiple)`

`Login to [iface: default, target: iqn.2014-03.com.example:b36d96e3-9136-44a3-8bc9-78bd2754a137` `,portal: 192.168.122.1,3260] successful.`

如您所见，因为您这次是通过正确的入口进入的，所以您将获得一个连接。在本例中，由于 iSCSI 目标绑定到 IP 地址 0.0.0.0，您将获得多个连接，每个 IP 地址一个。

此时，您可以验证连接。一个简单的方法是使用`lsscsi`命令。

`node1:/etc/init.d #` `lsscsi`

`[0:0:0:0]    cd/dvd  QEMU     QEMU DVD-ROM     0.15  /dev/sr0`

`[2:0:0:0]    disk    IET      VIRTUAL-DISK     0     /dev/sda`

如您所见，添加了一个磁盘类型为 IET 的虚拟磁盘`/dev/sda`。你现在连接到 iSCSI 目标！如果在您的运行级别中启用了 iSCSI 支持服务，重新启动时也会自动重新建立 iSCSI 连接。

为了自动重新建立所有 iSCSI 会话，iSCSI 启动器将其已知配置写入`$ISCSI-ROOT/nodes`。在这个目录中，您将找到一个以目标的 IQN 的名称作为其名称的子目录。在该子目录中，您还可以找到服务器连接到的每个门户的子目录，在该子目录中，您可以找到默认文件，其中包含用于连接到 iSCSI 目标的设置。

`node1:/etc/iscsi/nodes # ls`

`iqn.2014-03.com.example:b36d96e3-9136-44a3-8bc9-78bd2754a137`

`node1:/etc/iscsi/nodes #` `cd iqn.2014-03.com.example\:b36d96e3-9136-44a3-8bc9-78bd2754a137/`

`node1:/etc/iscsi/nodes/iqn.2014-03.com.example:b36d96e3-9136-44a3-8bc9-78bd2754a`

`137 #` `ls`

`192.168.122.1,3260,1  192.168.178.36,3260,1`

`node1:/etc/iscsi/nodes/iqn.2014-03.com.example:b36d96e3-9136-44a3-8bc9-78bd2754a`

`137 #` `cd 192.168.122.1,3260,1/`

`node1:/etc/iscsi/nodes/iqn.2014-03.com.example:b36d96e3-9136-44a3-8bc9-78bd2754a`

`137/192.168.122.1,3260,1 #` `ls`

`default`

此配置可确保您在重新启动时重新建立完全相同的 iSCSI 会话。

### 步骤 3:管理 iSCSI 连接

既然您已经使用了`iscsiadm --mode node`命令来建立连接，那么您可以做不同的事情来管理这个连接。首先，让我们看看当前的连接信息，使用`iscsiadm --mode node -P 1`。下面给出了现有的当前目标连接的摘要:

`node1:∼ #` `iscsiadm --mode node -P 1`

`Target: iqn.2014-03.com.example:b36d96e3-9136-44a3-8bc9-78bd2754a137`

`Portal: 192.168.178.36:3260,1`

`Iface Name: default`

`Portal: 192.168.122.1:3260,1`

`Iface Name: default`

要获得关于当前设置的更多信息，包括在默认文件中为每个会话定义的性能参数，您可以使用`iscsiadm --mode session -P 1`，如下所示:

`node1:∼ #` `iscsiadm --mode session -P 2`

`Target: iqn.2014-03.com.example:b36d96e3-9136-44a3-8bc9-78bd2754a137`

`Current Portal: 192.168.122.1:3260,1`

`Persistent Portal: 192.168.122.1:3260,1`

`**********`

`Interface:`

`**********`

`Iface Name: default`

`Iface Transport: tcp`

`Iface Initiatorname: iqn.1996-04.de.suse:01:77766ea5aae2`

`Iface IPaddress: 192.168.122.130`

`Iface HWaddress: <empty>`

`Iface Netdev: <empty>`

`SID: 1`

`iSCSI Connection State: LOGGED IN`

`iSCSI Session State: LOGGED_IN`

`Internal iscsid Session State: NO CHANGE`

`*********`

`Timeouts:`

`*********`

`Recovery Timeout: 120`

`Target Reset Timeout: 30`

`LUN Reset Timeout: 30`

`Abort Timeout: 15`

`*****`

`CHAP:`

`*****`

`username: <empty>`

`password: ********`

`username_in: <empty>`

`password_in: ********`

`************************`

`Negotiated iSCSI params:`

`************************`

`HeaderDigest: None`

`DataDigest: None`

`MaxRecvDataSegmentLength: 262144`

`MaxXmitDataSegmentLength: 8192`

`FirstBurstLength: 65536`

`MaxBurstLength: 262144`

`ImmediateData: Yes`

`InitialR2T: Yes`

`MaxOutstandingR2T: 1`

### 断开 iSCSI 会话

如前所述，iSCSI 设置为在服务器重新启动时重新建立所有会话。如果您的配置发生变化，您可能需要删除该配置。为此，您必须删除会话信息。首先，您必须断开连接，这也意味着从 iSCSI 目标服务器的角度来看，连接已经断开。要断开一个会话，您将使用`iscsiadm --mode node --logout`。这将断开您与所有 iSCSI 磁盘的连接，从而允许您在 iSCSI 存储区域网络上进行维护。如果在重新启动后，您还希望不要自动重建 iSCSI 会话，最简单的方法是删除`$ISCSI_ROOT/node`目录的全部内容。因为在重启时，iSCSI 服务找不到任何配置；你可以从头再来。

## 设置多路径

通常，存储区域网络(SAN)拓扑以冗余方式建立。这意味着，即使控制器、磁盘、网络连接或 SAN 上的任何东西出现故障，服务器与存储设备的连接也不会受到影响。这还意味着，如果您通过多个连接来连接到 SAN，SAN 上的逻辑单元号(LUN)将会出现多次。如果您的 LUN 有四条不同的路径，在连接的节点上，您将看到`/dev/sda`、`/dev/sdb`、`/dev/sdc`以及`/dev/sdd`，它们都指向同一个设备。

由于所有的`/dev/sd`设备都绑定到一个特定的路径，所以您不应该连接到它们中的任何一个。如果你当时连接的特定路径出现故障，你将失去连接。这就是发明多路径的原因。

多路径是加载的驱动程序，它分析所有存储设备。它会发现设备`/dev/sda`、`/dev/sdb`、`/dev/sdc`和`/dev/sdd`都指向同一个 LUN，因此，它会创建一个您可以连接的特定设备。让我们看看这在一个示例服务器上是什么样子的。

首先，`iscsiadm -m session -P 1`命令显示存在两个不同的 SAN 连接，使用不同的接口和不同的 IP 地址。

`[root@apache2 ∼]# iscsiadm -m session -P 1`

`Target: iqn.2001-05.com.equallogic:0-8a0906-48578f104-b07002fe41053218-sharedmoodle2`

`Current Portal: 192.168.50.126:3260,1`

`Persistent Portal: 192.168.50.121:3260,1`

`**********`

`Interface:`

`**********`

`Iface Name: p1p1`

`Iface Transport: tcp`

`Iface Initiatorname: iqn.1994-05.com.redhat:33dbb91a277a`

`Iface IPaddress: 192.168.50.103`

`Iface HWaddress: <empty>`

`Iface Netdev: p1p1`

`SID: 1`

`iSCSI Connection State: LOGGED IN`

`iSCSI Session State: LOGGED_IN`

`Internal iscsid Session State: NO CHANGE`

`Current Portal: 192.168.50.16:3260,1`

`Persistent Portal: 192.168.50.121:3260,1`

`**********`

`Interface:`

`**********`

`Iface Name: p1p2`

`Iface Transport: tcp`

`Iface Initiatorname: iqn.1994-05.com.redhat:33dbb91a277a`

`Iface IPaddress: 192.168.50.198`

`Iface HWaddress: <empty>`

`Iface Netdev: p1p2`

`SID: 2`

`iSCSI Connection State: LOGGED IN`

`iSCSI Session State: LOGGED_IN`

`Internal iscsid Session State: NO CHANGE`

当在那个主机上使用`lsscsi`时，您可以看到有一个`/dev/sdb`和一个`/dev/sdc`。因此，在这种情况下，有两条不同的路径通向 SAN。

`[root@apache2 ∼]# iscsiadm -m session -P 1`

`Target: iqn.2001-05.com.equallogic:0-8a0906-48578f104-b07002fe41053218-sharedmoodle2`

`Current Portal: 192.168.50.126:3260,1`

`Persistent Portal: 192.168.50.121:3260,1`

`**********`

`Interface:`

`**********`

`Iface Name: p1p1`

`Iface Transport: tcp`

`Iface Initiatorname: iqn.1994-05.com.redhat:33dbb91a277a`

`Iface IPaddress: 192.168.50.103`

`Iface HWaddress: <empty>`

`Iface Netdev: p1p1`

`SID: 1`

`iSCSI Connection State: LOGGED IN`

`iSCSI Session State: LOGGED_IN`

`Internal iscsid Session State: NO CHANGE`

`Current Portal: 192.168.50.16:3260,1`

`Persistent Portal: 192.168.50.121:3260,1`

`**********`

`Interface:`

`**********`

`Iface Name: p1p2`

`Iface Transport: tcp`

`Iface Initiatorname: iqn.1994-05.com.redhat:33dbb91a277a`

`Iface IPaddress: 192.168.50.198`

`Iface HWaddress: <empty>`

`Iface Netdev: p1p2`

`SID: 2`

`iSCSI Connection State: LOGGED IN`

`iSCSI Session State: LOGGED_IN`

`Internal iscsid Session State: NO CHANGE`

在此服务器上，加载了多路径驱动程序。要检查当前拓扑，您可以使用`multipath -l`命令。

`[root@apache2 ∼]# multipath -l`

`mpatha (36090a048108f574818320541fe0270b0) dm-2 EQLOGIC,100E-00`

`size=700G features='0' hwhandler='0' wp=rw`

`|-+- policy='round-robin 0' prio=0 status=active`

`| `- 7:0:0:0 sdb 8:16 active undef running`

``-+- policy='round-robin 0' prio=0 status=enabled`

``- 8:0:0:0 sdc 8:32 active undef running`

如您所见，已经创建了一个名为`mpatha`的新设备。该设备创建在运行多路径服务的集群节点上的`/dev/mapper`目录中。您还可以看到它正在使用`round-robin`连接到底层设备`sdb`和`sdc`。其中，一个的状态设置为`active`，另一个的状态设置为`enabled`。

此时，群集节点将通过`/dev/mapper/mpatha`设备寻址 SAN 存储。如果在连接过程中，其中一个底层路径出现故障，也没关系。多路径驱动程序自动切换到剩余的设备。

### /etc/multipath.conf

启动多路径服务时，会使用配置文件。在此配置文件中，可以指定关于多路径设备的不同设置。在下面的清单中，您可以看到该文件的内容可能是什么样子:

`#blacklist {`

`#       wwid 26353900f02796769`

`#       devnode "^(ram|raw|loop|fd|md|dm-|sr|scd|st)[0-9]*"`

`#       devnode "^hd[a-z]"`

`#}`

`#multipaths {`

`#       multipath {`

`#               wwid                    3600508b4000156d700012000000b0000`

`#               alias                   yellow`

`#               path_grouping_policy    multibus`

`#               path_checker            readsector0`

`#               path_selector           "round-robin 0"`

`#               failback                manual`

`#               rr_weight               priorities`

`#               no_path_retry           5`

`#       }`

`#       multipath {`

`#               wwid                    1DEC_____321816758474`

`#               alias                   red`

`#       }`

`#}`

`#devices {`

`#       device {`

`#               vendor                  "COMPAQ"`

`#               product                 "HSV110 (C)COMPAQ"`

`#               path_grouping_policy    multibus`

`#               getuid_callout          "/lib/udev/scsi_id --whitelisted --device=/dev/%n"`

`#               path_checker            readsector0`

`#               path_selector           "round-robin 0"`

`#               hardware_handler        "0"`

`#               failback                15`

`#               rr_weight               priorities`

`#               no_path_retry           queue`

`#       }`

`#       device {`

`#               vendor                  "COMPAQ"`

`#               product                 "MSA1000"`

`#               path_grouping_policy    multibus`

`#       }`

`#}`

`#`

`defaults {`

`udev_dir                /dev`

`find_multipaths         yes`

`user_friendly_names     yes`

`}`

`multipaths {`

`multipath {`

`wwid 36090a048108f574818320541fe0270b0`

`alias mpatha`

`}`

`}`

在前面的清单中，使用了不同的参数。首先，有一个黑名单部分(被注释掉)。在此部分，您可以排除特定设备。如果您使用的 SAN 硬件有自己的多路径驱动程序，并且不应该使用通用的 Linux 多路径驱动程序，这是有意义的。列入黑名单时，您可以使用全球 ID (WWID)来指代应该排除的特定设备。此外，在黑名单部分，您可以看到被排除的`devnode`列表。该列表通常包含本地设备，因为您不希望在本地设备上执行任何多路径操作。

在配置文件的末尾，您可以看到在这个配置中实际有效的设置。它以默认值开始，指示使用哪个目录来创建多路径设备的设备文件。接下来，它指示多路径驱动程序使用用户友好的名称。另一个重要部分是设置别名的位置。此别名基于 WWID，它是多路径设备的唯一 ID。如果您什么都不做，您将只有一个通用设备映射器设备名称，如`/dev/dm-1`，指的是多路径设备。

因为`/dev/dm-*`名称是本地设置的，并且在集群中的不同节点上可能不同，所以它们可能永远不会被使用。这就是为什么使用 WWID 来为多路径设备设置别名的原因。要找出要使用的 WWID，请执行以下步骤:

Make sure all of the SAN connections are operational.   Start the multipath service, using a command such as `systemctl start multipath.service` (it might be different on your distribution).   Type `multipath -l` to find the current WWIDs and identify which specific ID is used on which specific LUNs.   Decide which alias to use and create the configuration in `/etc/multipath.conf`.  

### 多路径的具体使用案例

在有两个不同接口的存储区域网络(SAN)上设置多路径很容易。然而，一些现代 SAN 使用 SAN 上的虚拟接口，其中 SAN 在内部处理冗余。在这种配置中，您可以通过冗余路径将群集节点连接到 SAN，但它会在两条路径上获得相同的信息，结果是您的群集节点看不到实际上有两条路径。在光纤通道 SAN 上，这通常由 HBA 来处理，不会有任何问题。但是，在 iSCSI SAN 上，这可能会导致第二条路径被完全忽略。因此，实际上会有多条路径，其中只有第一条路径被使用。这意味着，如果该特定路径出现故障，将失去与存储的连接。为了处理这些特定的情况，您必须以特定的方式建立 iSCSI 连接。

让我们来看看到底是什么问题。在图 [2-5](#Fig5) 中，您可以看到配置的示意图。这种情况很容易在测试环境中模拟。只需确保一个群集节点在同一个 IP 子网中有两个具有不同 IP 地址的物理接口。接下来，尝试使用`iscsiadm`命令连接到 SAN，如上所述，使用`iscsiadm -m discoverydb --type discovery --portal 192.168.50.121:3260 --discover`和`iscsiadm -m node --targetname iqn.2013-03.com.example:apache`T3。您会注意到，您只建立了一个会话。

![A978-1-4842-0079-7_2_Fig5_HTML.jpg](A978-1-4842-0079-7_2_Fig5_HTML.jpg)

图 2-5。

Partial multipathing configuration

以下过程描述了如何以正确的方式建立 iSCSI 连接，从而允许您在真正冗余的配置上工作。

Log out from all existing sessions: `iscsiadm --mode node --targetname iqn.2013-03.com.example:apache` `--portal 192.168.50.121:3260 --logout`   Stop the `iscsid` service and remove all current session configuration files: `systemctl stop iscsid.service; rm -rf $ISCSI_ROOT/nodes $ISCSI_ROOT/ifaces`   Now, you have to define different interfaces in iSCSI. This tells iSCSI that each interface should be dealt with separately.  

`iscsiadm --mode iface --interface p2p1 -o new`

`iscsiadm --mode iface --interface p2p2 -o new`

After defining the interfaces, you must write the interface settings. These settings are written to the `$ISCSI_ROOT/ifaces` directory and allow iSCSI to distinguish between the interfaces.  

`iscsiadm --mode iface --interface p2p1 -o update --name \ iface.net_ifacename --value=p2p1`

`iscsiadm --mode iface --interface p2p2 -o update --name \ iface.net_ifacename --value=p2p2`

Before getting operational, you’ll have to tell the kernel of each node that it can accept packets that are sent to addresses in the same IP network on different interfaces. To prevent spoof attacks, this is off by default, which means that the kernel accepts packets to a specific IP subnet on one interface only. To change this behavior, add the following line to the end of `/etc/sysctl.conf` and reboot:  

`net.ipv4.conf.default/rp_filter = 2`

After the reboot, you can start the iSCSI discovery.  

`iscsiadm -m discoverydb -t sendtarets -p 192.168.50.121:3260 --discover`

因此，您将看到两个连接，每个接口一个。您还可能会看到错误“无法扫描/sys/class/iscsi_transport”当您第一次扫描新接口时，这个错误是完全正常的，所以您可以放心地忽略它。

Now, you can log in to the SAN with the command `iscsiadm -m node -l`.   At this point, you can start the multipath driver, using `systemctl start multipathd.service`.   If you now type `multipath -l`, you’ll see that the multipath devices have been created properly.  

## 摘要

在任何高可用性群集中，连接到共享存储都很重要。在本章中，您已经了解了可用的不同种类的共享存储以及如何连接到它们。特别是，您已经学习了如何使用`iscsiadm`命令连接到 iSCSI SAN。此外，您还学习了如何通过使用多路径驱动程序来保证 SAN 连接的冗余性。在下一章中，您将学习如何设置集群的较低层。