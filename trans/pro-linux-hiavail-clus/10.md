# 十、用例：创建 Xen/KVM 高可用性解决方案

在前面的章节中，您了解了如何将虚拟机用作集群节点，以确保重要资源的可用性最大化。但是，如果虚拟机本身停止运行，会发生什么情况呢？您可能正在使用 VMware，并且在 VMware 中配置了高可用性(HA ),这可确保虚拟机在停机时能够自动重启。在这一章中，你将学习如何使用开源软件创建一个替代品。

本章涵盖了以下主题:

*   开源虚拟化解决方案
*   为虚拟机设置高可用性解决方案的要求
*   虚拟机高可用性集群示例
*   Xen 虚拟机的配置
*   KVM(基于内核的虚拟机)的配置

## 简介:开源虚拟化解决方案概述

VMware 正在主导虚拟化解决方案市场。但是如果你运行的只是 Linux 服务器，那么使用 VMware 就没有意义了。在 Linux 中，有不同的优秀虚拟化解决方案:KVM、Xen 和 Linux 容器。所有这些的优点是，该技术包含在现代 Linux 发行版中，因此该解决方案是免费的。

### 虚拟机

在三个领先的 Linux 虚拟化解决方案中，Xen 是最老的。它在 2006 年左右开始流行，引入了一些在虚拟化领域前所未见的解决方案，Xen hypervisor 包含在领先的 Linux 发行版中，如 SUSE 和 Red Hat。

从一开始，Xen 就很难被 Linux 内核采用。许多补丁被应用到 Linux 内核以实现 Xen 虚拟化，这也是 Xen 在 Linux 开发人员社区中没有变得非常流行的主要原因。当 Xen 创始人创建的公司被 Citrix 收购后，Xen 项目的受欢迎程度进一步下降，这使得 Xen 作为开源虚拟化平台的未来更加不确定。

然而，Citrix 并没有真正能够将 Xen 转化为重大成功，这导致该公司在很大程度上放弃了 Xen 虚拟化，以及基于 Xen 的商业解决方案 XenServer。由于这些事件，Linux 基金会接管 Xen 项目成为可能，这导致了 Xen 在 2013 年的复兴。

对于一些 Linux 发行版，Xen 仍然是一个重要的解决方案。SUSE 和 Oracle Linux 正在提供基于 Xen 的虚拟化解决方案，这是有充分理由的，因为一些重要的客户已经采用 Xen 作为他们的默认虚拟化解决方案。他们这样做是有原因的。Xen 现在已经存在很久了，解决方案也很稳定。随着 Linux 基金会接管 Xen，以及 SUSE Linux Enterprise Server 等仍在提供 Xen 支持的重要企业发行版，Xen 肯定有一个光明的未来。

### 千伏计（kilovoltmeter 的缩写）

基于内核的虚拟机(KVM)是 Linux 内核虚拟化。KVM 的最大优势在于它很简单:每个 Linux 内核都包含两个内核模块，这两个模块是设置 KVM 虚拟环境所必需的。要使用 KVM 虚拟化，您需要服务器 CPU 上的虚拟化硬件支持。默认情况下，大多数服务器级 CPU 都提供这种支持，但在更基本的 CPU 型号上，可能会缺少这种支持，这使得无法使用 KVM 虚拟化。

KVM 已经成为 Linux 上领先的基于虚拟机管理程序的虚拟化解决方案，尤其是随着 Red Hat Enterprise Linux 6 的推出，Red Hat 将该解决方案作为其默认和唯一的虚拟化解决方案。

## 为虚拟机设置高可用性解决方案的要求

为了设置虚拟机集群，必须设置一些特定的项目。首先，您需要一个允许多台主机同时访问虚拟机的解决方案。在大多数情况下，这是通过将虚拟机映像文件放在存储区域网络(SAN)上来实现的。接下来，主机需要连接到 SAN，以便能够同时访问虚拟机文件和写入虚拟机文件。有三种可能的解决方案。

*   您可以设置一个集群文件系统，如 GFS2 或 OCFS2。如果您希望使用映像文件而不是用于访问的原始存储设备来配置虚拟机，这种解决方案是有意义的。
*   您可以设置一个集群 LVM 卷组，其中包含一个所有节点都可以同时访问的逻辑卷。这种设置允许虚拟机使用存储设备作为其虚拟磁盘的后端的配置，这提供了更健壮和更快的配置。
*   您可以通过文件共享服务(如 NFS)安排同时访问。这是最不可取的解决方案，因为通过 NFS 服务器访问文件比直接访问文件慢，而且您还需要为 NFS 服务器本身设置一个高可用性解决方案。

理论上，您还可以在存储之上建立一个虚拟化集群，该集群在同一时间只能由一个节点访问。

## 虚拟机高可用性集群示例

为 Xen 虚拟机创建集群有不同的方法。清单 10-1 中的例子在实践中被用来保证 Xen 虚拟机的可用性。

清单 10-1。Xen 高可用性集群

`node xen-ha-01 \`

`attributes standby="off"`

`node xen-ha-02 \`

`attributes standby="off"`

`node xen-ha-03 \`

`attributes standby="off"`

`node xen-ha-04 \`

`attributes standby="off"`

`primitive clvmd ocf:lvm2:clvmd \`

`operations $id="clvmd-operations" \`

`op monitor interval="10" timeout="20" \`

`op start interval="0" timeout="90" \`

`op stop interval="0" timeout="100" \`

`params daemon_timeout="80"`

`primitive controld ocf:pacemaker:controld \`

`operations $id="controld-operations" \`

`op monitor interval="10" timeout="20" start-delay="0" \`

`op start interval="0" timeout="90" \`

`op stop interval="0" timeout="100"`

`primitive fs_shared-fs ocf:heartbeat:Filesystem \`

`operations $id="fs_shared-fs-operations" \`

`op monitor interval="20" timeout="40" \`

`op start interval="0" timeout="60" \`

`op stop interval="0" timeout="60" \`

`op notify interval="0" timeout="60" \`

`params device="/dev/shared-fs/shared-fsvol" directory="/shared-fs" fstype="ocfs2"`

`primitive lvm_activate ocf:heartbeat:LVM \`

`operations $id="lvm_activate-operations" \`

`op monitor interval="10" timeout="180" \`

`op start interval="0" timeout="120" \`

`op stop interval="0" timeout="120" \`

`params volgrpname="shared-fs" exclusive="false" partial_activation="true"`

`primitive o2cb ocf:ocfs2:o2cb \`

`operations $id="o2cb-operations" \`

`op monitor interval="10" timeout="20" \`

`op start interval="0" timeout="90" \`

`op stop interval="0" timeout="100"`

`primitive ping ocf:pacemaker:ping \`

`operations $id="ping-operations" \`

`op monitor interval="10" timeout="60" \`

`op start interval="0" timeout="60" \`

`params host_list="192.168.1.254" multiplier="1000"`

`primitive stonith_sbd stonith:external/sbd \`

`meta target-role="Started" \`

`operations $id="stonith_sbd-operations" \`

`op monitor interval="15" timeout="120" start-delay="15"`

`primitive xend lsb:xend \`

`operations $id="xend-operations" \`

`op monitor interval="15" timeout="30" \`

`op start interval="0" timeout="300" \`

`op stop interval="0" timeout="300"`

`primitive xen-vm-01 ocf:heartbeat:Xen \`

`meta target-role="Started" allow-migrate="true" \`

`operations $id="xen-vm-01-operations" \`

`op start interval="0" timeout="5400" \`

`op stop interval="0" timeout="5400" \`

`op monitor interval="300" timeout="300" start-delay="600" \`

`op migrate_to interval="0" timeout="3600" \`

`op migrate_from interval="0" timeout="120" \`

`params xmfile="/shared-fs/xen-ha/configs/xen-vm-01.cfg"`

`primitive xen-vm-02 ocf:heartbeat:Xen \`

`meta target-role="Started" allow-migrate="true" \`

`operations $id="xen-vm-02-operations" \`

`op start interval="0" timeout="5400" \`

`op stop interval="0" timeout="5400" \`

`op monitor interval="300" timeout="300" start-delay="600" \`

`op migrate_to interval="0" timeout="3600" \`

`op migrate_from interval="0" timeout="120" \`

`params xmfile="/shared-fs/xen-ha/configs/xen-vm-02.cfg"`

`primitive xen-vm-03 ocf:heartbeat:Xen \`

`meta target-role="Started" allow-migrate="true" \`

`operations $id="xen-vm-03-operations" \`

`op start interval="0" timeout="5400" \`

`op stop interval="0" timeout="5400" \`

`op monitor interval="300" timeout="300" start-delay="600" \`

`op migrate_to interval="0" timeout="3600" \`

`op migrate_from interval="0" timeout="120" \`

`params xmfile="/shared-fs/xen-ha/configs/xen-vm-03.cfg"`

`primitive xen-vm-04 ocf:heartbeat:Xen \`

`meta allow-migrate="true" target-role="Started" \`

`operations $id="xen-vm-04-operations" \`

`op start interval="0" timeout="5400" \`

`op stop interval="0" timeout="5400" \`

`op monitor interval="300" timeout="300" start-delay="600" \`

`op migrate_to interval="0" timeout="3600" \`

`op migrate_from interval="0" timeout="120" \`

`params xmfile="/shared-fs/xen-ha/configs/xen-vm-04.cfg"`

`primitive xen-vm-05 ocf:heartbeat:Xen \`

`meta target-role="Started" allow-migrate="true" \`

`operations $id="xen-vm-05-operations" \`

`op start interval="0" timeout="5400" \`

`op stop interval="0" timeout="5400" \`

`op monitor interval="300" timeout="300" start-delay="600" \`

`op migrate_to interval="0" timeout="3600" \`

`op migrate_from interval="0" timeout="120" \`

`params xmfile="/shared-fs/xen-ha/configs/xen-vm-05.cfg"`

`primitive xen-vm-06 ocf:heartbeat:Xen \`

`meta target-role="Started" allow-migrate="true" \`

`operations $id="xen-vm-06-operations" \`

`op start interval="0" timeout="5400" \`

`op stop interval="0" timeout="5400" \`

`op monitor interval="300" timeout="300" start-delay="600" \`

`op migrate_to interval="0" timeout="3600" \`

`op migrate_from interval="0" timeout="120" \`

`params xmfile="/shared-fs/xen-ha/configs/xen-vm-06.cfg"`

`primitive xen-vm-07 ocf:heartbeat:Xen \`

`meta target-role="Started" allow-migrate="true" \`

`operations $id="xen-vm-07-operations" \`

`op start interval="0" timeout="5400" \`

`op stop interval="0" timeout="5400" \`

`op monitor interval="300" timeout="300" start-delay="600" \`

`op migrate_to interval="0" timeout="3600" \`

`op migrate_from interval="0" timeout="120" \`

`params xmfile="/shared-fs/xen-ha/configs/xen-vm-07.cfg"`

`primitive xen-vm-08 ocf:heartbeat:Xen \`

`meta target-role="Started" allow-migrate="true" \`

`operations $id="xen-vm-08-operations" \`

`op start interval="0" timeout="5400" \`

`op stop interval="0" timeout="5400" \`

`op monitor interval="300" timeout="300" start-delay="600" \`

`op migrate_to interval="0" timeout="3600" \`

`op migrate_from interval="0" timeout="120" \`

`params xmfile="/shared-fs/xen-ha/configs/xen-vm-08.cfg"`

`primitive xen-vm-09 ocf:heartbeat:Xen \`

`meta target-role="Started" allow-migrate="true" \`

`operations $id="xen-vm-09-operations" \`

`op start interval="0" timeout="5400" \`

`op stop interval="0" timeout="5400" \`

`op monitor interval="300" timeout="300" start-delay="600" \`

`op migrate_to interval="0" timeout="3600" \`

`op migrate_from interval="0" timeout="120" \`

`params xmfile="/shared-fs/xen-ha/configs/xen-vm-09.cfg"`

`group storage_group controld clvmd o2cb lvm_activate fs_shared-fs`

`clone ping_clone ping \`

`meta target-role="Started" ordered="true"`

`clone storage_clone storage_group \`

`meta target-role="Started" ordered="true"`

`clone xend_clone xend \`

`meta target-role="Started" ordered="true"`

`location location_xen-vm-01 xen-vm-01 100: xen-ha-01`

`location location_xen-vm-02 xen-vm-02 100: xen-ha-01`

`location location_xen-vm-03 xen-vm-03 100: xen-ha-01`

`location location_xen-vm-04 xen-vm-04 100: xen-ha-02`

`location location_xen-vm-05 xen-vm-05 100: xen-ha-02`

`location location_xen-vm-06 xen-vm-06 100: xen-ha-03`

`location location_xen-vm-07 xen-vm-07 100: xen-ha-03`

`location location_xen-vm-08 xen-vm-08 100: xen-ha-04`

`location location_xen-vm-09 xen-vm-09 100: xen-ha-04`

`location ping_xen-vm-01 xen-vm-01 \`

`rule $id="ping_xen-vm-01-rule" -inf: pingd lte 0`

`location ping_xen-vm-02 xen-vm-02 \`

`rule $id="ping_xen-vm-02-rule" -inf: pingd lte 0`

`location ping_xen-vm-03 xen-vm-03 \`

`rule $id="ping_xen-vm-03-rule" -inf: pingd lte 0`

`location ping_xen-vm-04 xen-vm-04 \`

`rule $id="ping_xen-vm-04-rule" -inf: pingd lte 0`

`location ping_xen-vm-05 xen-vm-05 \`

`rule $id="ping_xen-vm-05-rule" -inf: pingd lte 0`

`location ping_xen-vm-06 xen-vm-06 \`

`rule $id="ping_xen-vm-06-rule" -inf: pingd lte 0`

`location ping_xen-vm-07 xen-vm-07 \`

`rule $id="ping_xen-vm-07-rule" -inf: pingd lte 0`

`location ping_xen-vm-08 xen-vm-08 \`

`rule $id="ping_xen-vm-08-rule" -inf: pingd lte 0`

`location ping_xen-vm-09 xen-vm-09 \`

`rule $id="ping_xen-vm-09-rule" -inf: pingd lte 0`

`colocation storage_clone-with-xend_clone inf: storage_clone xend_clone`

`colocation xend_clone-with-ping_clone inf: xend_clone ping_clone`

`order storage_clone-after-xend_clone 0: xend_clone storage_clone`

`order xend_clone-after-ping_clone 0: ping_clone xend_clone`

`order xen-vm-01-after-storage_clone 0: storage_clone xen-vm-01`

`order xen-vm-02-after-storage_clone 0: storage_clone xen-vm-02`

`order xen-vm-03-after-storage_clone 0: storage_clone xen-vm-03`

`order xen-vm-04-after-storage_clone 0: storage_clone xen-vm-04`

`order xen-vm-05-after-storage_clone 0: storage_clone xen-vm-05`

`order xen-vm-06-after-storage_clone 0: storage_clone xen-vm-06`

`order xen-vm-07-after-storage_clone 0: storage_clone xen-vm-07`

`order xen-vm-08-after-storage_clone 0: storage_clone xen-vm-08`

`order xen-vm-09-after-storage_clone 0: storage_clone xen-vm-09`

`property $id="cib-bootstrap-options" \`

`dc-version="1.1.7-77eeb099a504ceda05d648ed161ef8b1582c7daf" \`

`cluster-infrastructure="openais" \`

`expected-quorum-votes="4" \`

`no-quorum-policy="ignore" \`

`stonith-action="poweroff" \`

`default-resource-stickiness="INFINITY" \`

`stonith-enabled="true" \`

`symmetric-cluster="true" \`

`stonith-timeout="60s" \`

`maintenance-mode="false" \`

`last-lrm-refresh="1384353701"`

此配置用于四节点集群，其中使用 cLVM 逻辑卷上 OCFS2 共享文件系统的资源。在这个配置中，您可以看到一些以前没有讨论过的资源代理和参数的使用。

首先是`ping`资源。

`primitive ping ocf:pacemaker:ping \`

`operations $id="ping-operations" \`

`op monitor interval="10" timeout="60" \`

`op start interval="0" timeout="60" \`

`params host_list="192.168.1.254" multiplier="1000"`

此资源用作帮助资源，用于监控节点是否仍连接到网络。`ping`资源周期性地 pings 一个应该一直可用的节点，在本例中是默认网关。这个资源的秘密在于它被用在一个约束中，这在`ping`资源和约束中的其他资源之间创建了一个依赖关系。在这种配置中，如果`ping`资源测试失败并生成退出代码 0，它也会使该节点上的依赖资源失败，从而迫使资源迁移到另一个节点。当使用`ping`资源时，确保始终使用参数`multiplier=1000`，否则它将无法工作。请注意，`ping`资源被配置为克隆，这确保它在集群中的所有主机上启动。

对于虚拟机本身，使用了两个原语(见清单 10-2)。首先是`xend`原语。`xend`是管理 Xen 虚拟机所需的接口，它必须在任何虚拟机启动之前可用。`xend`是一种相对简单的资源，这也是该资源被配置为 LSB 资源的原因。接下来，定义 Xen 虚拟机。

清单 10-2。Xen HA 所需的资源

`primitive xend lsb:xend \`

`operations $id="xend-operations" \`

`op monitor interval="15" timeout="30" \`

`op start interval="0" timeout="300" \`

`op stop interval="0" timeout="300"`

`primitive xen-vm-01 ocf:heartbeat:Xen \`

`meta target-role="Started" allow-migrate="true" \`

`operations $id="xen-vm-01-operations" \`

`op start interval="0" timeout="5400" \`

`op stop interval="0" timeout="5400" \`

`op monitor interval="300" timeout="300" start-delay="600" \`

`op migrate_to interval="0" timeout="3600" \`

`op migrate_from interval="0" timeout="120" \`

`params xmfile="/shared-fs/xen-ha/configs/xen-vm-01.cfg"`

Xen 虚拟机的 OCF 资源有一些特定的属性。首先，它们需要元属性`allow-migrate="true"`。此参数是允许实时迁移所必需的。另外，请注意 Xen RA 有一个`migrate to`和一个`migrate from`参数，它定义了从当前位置迁移和迁移到当前位置的超时。接下来是`xmfile`参数，它指定 Xen 虚拟机的配置文件可以在哪里找到。

因为所有虚拟机都在一个首选节点上启动是有意义的，所以有几个位置约束，每个都有 100 分。这表达了对在其首选节点上运行每个资源的轻微偏好。

最后，还有通用的集群属性。一个有趣的参数是`no-quorum-policy`，它被设置为`ignore`。这意味着在仲裁丢失的情况下，资源仍然可以迁移到剩余的节点。这背后的想法是，两个节点应该足以运行所有虚拟机，并且该集群中的 STONITH 以一种保证永远不会发生裂脑情况的方式设置。另外，注意`default-resource-stickiness`，它被设置为`INFINITY`。这确保了资源将保持在它们原来的位置，而不会自动迁移，这对于服务的可用性来说是非常不利的。

## 创建 KVM HA 集群

许多小型环境使用 KVM 来实现高可用性—不是像 oVirt 这样的托管解决方案中的 KVM，而是普通的 KVM，其中虚拟机直接运行在集成在 Linux 操作系统中的虚拟机管理程序之上。在太多的情况下，它只是 KVM，没有采取任何措施来确保业务在主机停机时继续运行。在本节中，您将了解如何提供一个简单的解决方案来确保 KVM 虚拟机的可用性。

KVM 随 Linux 内核一起提供，因此您可以在任何 Linux 发行版上使用它(图 [10-1](#Fig1) )。对于 KVM 部分，不会有太多区别。但是，对于集群部分，会有。即使集群在大多数 Linux 发行版上或多或少都是一样的，但还是有一些小的差异很重要。由于 Pacemaker stack 起源于 SUSE 发行版，而 Red Hat 只是在最近的版本中完成了它的解决方案，所以在本节中，我更愿意解释 SUSE 配置。当试图在 Red Hat 上配置本节中描述的设置时，仍然有一些粗糙的边缘，使您无法灵活地配置最适合您需求的解决方案。在 SUSE 上，一切正常。使用的版本是完全打了补丁的 OpenSUSE 13.1。

![A978-1-4842-0079-7_10_Fig1_HTML.jpg](img/A978-1-4842-0079-7_10_Fig1_HTML.jpg)

图 10-1。

KVM virtualization overview

这里描述的过程假设节点已经配置到存储区域网络(SAN)。如果不是这样，将虚拟化主机连接到 Linux SAN 就相对容易了，Linux SAN 根据 LIO 或 IET iSCSI 的目标提供共享存储。当然，如果您的环境中有 SAN 设备，您也可以使用它。但是，请确保您使用的是 SAN，而不是网络连接存储(NAS)。基于 NAS 的高可用性对于虚拟机是可能的，但出于性能和可靠性原因，不建议这样做。此外，在下面的过程中，您将学习如何使用 OCFS2 共享文件系统构建集群，这仅在您位于 SAN 上时才有效。

要配置 KVM HA 集群，必须执行以下步骤:

Create the base cluster.   Configure an OCFS2 cluster file system on the SAN.   Install a KVM virtual machine using the SAN disk as the storage back end.   Set up Pacemaker cluster resources for the KVM virtual machine.   Verify the cluster configuration.  

### 创建基本集群

要在 OpenSUSE 13.1 上创建基本集群，您必须执行以下步骤:

Use `zypper in pacemaker ocfs2-tools lvm2-clvm` to install all of the packages required to build the cluster.   The cluster consists of two layers. The lower layer takes care of communications in the cluster and is called Corosync. The upper layer takes care of resource management. To configure the lower layer, a good sample configuration file is provided with the name `/etc/corosync/corosync.conf.example`. Copy this file to the file `/etc/corosync/corosync.conf` and make sure to modify the following lines:  

`bindnetaddr: 192.168.4.0`

`quorum {`

`# Enable and configure quorum subsystem (default: off)`

`# see also corosync.conf.5 and votequorum.5`

`provider: corosync_votequorum`

`expected_votes: 2`

`}`

`bindnetaddr`行应该反映您的节点用来在网络上通信的 IP 网络地址。仲裁行告诉集群预期有多少个节点。

Start and enable the Corosync and Pacemaker services, using `systemctl start corosync` ; `systemctl start pacemaker; systemctl enable corosync` ; `systemctl enable pacemaker`.   Type `crm_mon`. This should give an output as in Listing 10-1, which verifies that the cluster is operational.  

`Last updated: Sun May 11 19:38:24 2014`

`Last change: Sun May 11 19:38:24 2014 by root via cibadmin on suse1`

`Stack: corosync`

`Current DC: suse1 (3232236745) - partition with quorum`

`Version: 1.1.10-1.2-d9bb763`

`2 Nodes configured`

`0 Resources configured`

`Online: [ suse1 suse2 ]`

### 为共享存储配置 SAN

要设置 OCFS2 共享文件系统，首先必须在集群上启动一些支持服务。键入`crm configure edit`并确保将以下几行添加到文件中:

`primitive dlm ocf:pacemaker:controld \`

`op start interval="0" timeout="90" \`

`op stop interval="0" timeout="100" \`

`op monitor interval="10" timeout="20" start-delay="0"`

`primitive o2cb ocf:ocfs2:o2cb \`

`op stop interval="0" timeout="100" \`

`op start interval="0" timeout="90" \`

`op monitor interval="20" timeout="20"`

`group ocfs2-base-group dlm o2cb`

`clone ocfs2-base-clone ocfs2-base-group \`

`meta ordered="true" clone-max="2" clone-node-max="1"`

`property $id="cib-bootstrap-options" \`

`cluster-infrastructure="corosync" \`

`stonith-enabled="false"`

启动这些基本服务后，您可以创建 ocfs2 文件系统。为此，键入`mkfs.ocfs2 /dev/sdb`。接下来，在两个节点上创建一个名为`/shared`的目录，并再次键入`crm configure edit`。此时，将以下内容添加到集群配置中:

`primitive ocfs-fs ocf:heartbeat:Filesystem \`

`params fstype="ocfs2" device="/dev/disk/by-path/ip-192.168.1.125:3260-iscsi-iqn.2014-01.com.example:kiabi" directory="/shared" \`

`op stop interval="0" timeout="60" \`

`op start interval="0" timeout="60" \`

`op monitor interval="20" timeout="40"`

`clone ocfs-fs-clone ocfs-fs \`

`meta clone-max="2" clone-node-max=1`

`order ocfs2-fs-after-ocfs-base 1000: ocfs2-base-clone ocfs-fs`

现在，在这两个节点上，您应该有一个可用的共享文件系统，并挂载在`/shared`目录中。写入一个节点的文件将在另一个节点上立即可见和可访问，这正是为虚拟机设置高可用性环境所需要的。

### 安装 KVM 虚拟机

要安装 KVM 虚拟机，虚拟化主机必须运行`libvirt`服务。使用`systemctl start libvirtd`；`systemctl enable libvirtd`为此。如果是这种情况，有两种开始安装的解决方案:您可以使用 Virtual Machine Manager 图形工具，也可以使用`virt-install`。用`virt-manager`命令启动的虚拟机管理器需要 X-server 来显示它的图形窗口，因此可能不可行。如果没有可用的图形环境，那么`virt-manager`实用程序也很有用，它允许您以非交互的方式从脚本环境中创建虚拟机。

要使用`virt-install`启动 VM 安装，可以使用如下命令:

`virt-install --name smallcent --ram 512 --disk path=/shared/smallcent.img,size=4 --network network:default --vnc --cdrom /isos/CentOS-6.5-x86_64-bin-DVD1.iso`

通过此命令，可以指定虚拟机的所有属性。虚拟机的名称被设置为`smallcent`。这个名称很重要，因为在为虚拟机创建集群资源时必须使用它。分配 512 MB RAM，在目录`/shared`下创建一个大小为 4GB 的磁盘文件。请注意，该目录应该位于在此过程的上一步中创建的 OCFS2 卷上。

对于网络，使用默认配置。下一个参数很重要，因为它会在虚拟机上打开一个 VNC 查看器，允许以交互方式完成虚拟机的安装。最后一个参数指的是用于完成 VM 安装的 CD-ROM 设备。

在此设置中，将启动交互式安装。在某些情况下，这是不可行的，因为没有终端连接到虚拟化主机。在这种情况下，必须使用自动化安装。涵盖这样的安装超出了本书的范围。关于如何设置 AutoYast (SUSE)或 Kickstart (Red Hat)服务器的指导，请参考您的发行版文档。

### 为 KVM 虚拟机设置集群资源

要在集群中集成虚拟机，您必须使虚拟机的配置对集群可用。为此，您必须将虚拟机的 XML 配置转储到一个文本文件中。首先，使用`virsh list --all`来验证虚拟机的名称。在本例中，虚拟机的名称是`smallcent`。因为集群需要访问包含虚拟机定义的 XML 文件，所以您必须将其转储到您之前设置的共享存储设备上的一个文件中。为此，键入`virsh dumpxml smallcent > /shared/smallcent.xml`。

此时，您可以为集群中的虚拟机创建资源。`VirtualDomain`资源代理用于此目的。使用`crm configure edit`并包括如下所示的配置:

`primitive smallcent ocf:pacemaker:VirtualDomain \`

`params hypervisor="qemu:///system" migration_transport="ssh" config="/shared/smallcent.xml" \`

`meta allow-migrate="true" \`

`op stop timeout="120" interval="0" \`

`op start timeout="120" interval="0" \`

`op monitor interval="20" timeout="20"`

请注意，为了使集群能够管理资源，必须从集群中的所有节点使用包含配置的 XML 文件。因此，您必须确保将其放在共享存储设备上。在前面的`pcs`命令中，您将使用`VirtualDomain`资源代理创建一个名为`smallcent`的资源。这个资源代理必须知道在哪里可以找到 hypervisor，这是通过在资源定义中包含`hypervisor="qemu://system"`来实现的。为了允许这个虚拟机的迁移，`migration_transport`机制被定义为`ssh`。请注意，只有当主机配置了允许从一台主机自动登录到另一台主机的密钥时，这种方法才有效。接下来，您必须指出集群在哪里可以找到用于管理资源的 XML 配置。

此时，如`crm configure edit`所示，配置应该类似于下面的列表:

`node $id="3232236745" suse1`

`node $id="3232236746" suse2`

`primitive dlm ocf:pacemaker:controld \`

`op start interval="0" timeout="90" \`

`op stop interval="0" timeout="100" \`

`op monitor interval="10" timeout="20" start-delay="0"`

`primitive o2cb ocf:ocfs2:o2cb \`

`op stop interval="0" timeout="100" \`

`op start interval="0" timeout="90" \`

`op monitor interval="20" timeout="20"`

`primitive smallcent ocf:pacemaker:VirtualDomain \`

`params hypervisor="qemu:///system" migration_transport="ssh" config="/shared/smallcent.xml" \`

`meta allow-migrate="true" \`

`op stop timeout="120" interval="0" \`

`op start timeout="120" interval="0" \`

`op monitor interval="20" timeout="20"`

`group ocfs2-base-group dlm o2cb`

`clone ocfs2-base-clone ocfs2-base-group \`

`meta ordered="true" clone-max="2" clone-node-max="1"`

`property $id="cib-bootstrap-options" \`

`dc-version="1.1.10-1.2-d9bb763" \`

`cluster-infrastructure="corosync" \`

`stonith-enabled="false" \`

`last-lrm-refresh="1399852426"`

`#vim:set syntax=pcmk`

您现在可以使用`crm_mon`命令来验证配置的工作情况。如果所有的配置都正确，您现在应该有一个完全运行的 KVM 高可用性集群。

## 摘要

在本章中，您已经学习了如何为虚拟机设置高可用性集群。下一章也是最后一章将向您展示如何建立一个复杂的集群，在这个集群中，配置了高可用性的 web 服务器和数据库服务器协同工作。