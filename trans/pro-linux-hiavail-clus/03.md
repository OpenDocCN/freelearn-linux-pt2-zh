# 3.配置成员层

为了使节点能够彼此看到，您必须配置集群成员层。这一层包括节点用于通信的基础设施，以及让节点实际通信的软件层。本章说明如何配置成员层。讨论了以下主题:

*   配置网络
*   处理多播
*   `corosync`还是`cman`？
*   配置`corosync`
*   配置`cman`

## 配置网络

在开始考虑软件的配置之前，您必须设置物理网络，并且有几个选择要做。首先，你必须决定你想要使用哪个网络。选择是使用局域网还是使用专用的集群网络。

对于测试环境，通过 LAN 发送集群流量是可以接受的。对于生产网络，您不应该这样做。这是因为集群流量是敏感的，重要的决策是基于集群流量的结果做出的。如果数据包没有通过，群集将得出一个节点已经消失的结论，并采取相应的行动。这意味着它将使用 STONITH 终止它不再看到的节点，然后将资源迁移到新的位置。这两者都涉及到使用资源的用户的停机时间，这就是为什么您需要一个专用的集群网络。

在群集网络上，您还需要保护网络连接。如果网卡出现故障或电缆断开，您不希望群集出现故障。这就是为什么您要配置网络绑定，也称为链路聚合。

在网络绑定中，创建一个逻辑接口将两个(或更多)物理接口放在一起。如果物理接口处于绑定状态，它们不包含任何信息；他们只是被配置为绑定的奴隶。包含 IP 地址配置的是丁波接口。因此，客户端与绑定接口通信，绑定接口使用绑定内核模块在从属接口上分配负载。

### 网络绑定模式

配置网络绑定时，有不同的模式可供选择。默认模式是`balance-rr`，这是一种循环模式，网络数据包按顺序从第一个可用的网络接口传输到最后一个。这种模式提供负载平衡和容错。在某些 SAN 过滤器上，不推荐使用循环法，因为根据供应商的说法，它会导致数据包丢失。在这种情况下，链路聚合控制协议(LACP)通常更受青睐。然而，没有交换机的支持，LCAP 就无法工作。简单循环法的优点是它不需要任何额外的配置。

表 [3-1](#Tab1) 概述了在 Linux 上使用绑定时可用的模式。

表 3-1。

Linux Bonding Modes

<colgroup><col> <col></colgroup> 
| 方式 | 使用 |
| --- | --- |
| `balance-rr` | 这是一种循环模式，数据包按顺序从第一个网络接口传输到最后一个网络接口。 |
| `active-backup` | 在这种模式下，只有一个从机是活动的，如果活动的从机发生故障，另一个从机将接管工作。 |
| `balance-xor` | 一种提供负载平衡和容错的模式，其中每个目的 MAC 地址使用同一个从设备。 |
| `broadcast` | 该模式仅提供容错，并在所有从机接口上广播数据包。 |
| `802.3ad` | 这是创建聚合组的 LACP 模式，在该模式下，所有从属设备使用相同的速度和双工设置。它需要在交换机上进行额外的配置。 |
| `balance-tlb` | 在这种被称为自适应传输负载平衡的模式中，数据包根据负载在每个网络接口从设备上发出。指定的从接口接收传入流量。 |
| `balance-alb` | 这类似于`balance-tlb`的工作方式，但也对输入的数据包进行负载平衡。 |

### 配置绑定接口

配置一个 bond 接口并不太难，尽管在特定的 Linux 发行版上，具体的过程可能会有所不同。这里描述的过程基于 SUSE Linux Enterprise Server 11，也适用于 Red Hat Enterprise Linux 12。在最近发布的 SUSE Linux Enterprise Server 12 和 Red Hat Enterprise Linux 7 中，网络发生了相当大的变化。我推荐使用 SUSE 的 YaST 设置实用程序或 Red Hat `nm-tui`实用程序来设置这些发行版中的绑定。

第一步是为绑定创建一个接口配置文件。这通常会有名字`ifcfg-bond0`，你会在`/etc/sysconfig/network`(苏斯)或`/etc/sysconfig/network-scripts`(红帽)中找到它。在清单 3-1 中，您可以看到文件的样子。

清单 3-1。示例焊接配置文件

`san:∼ #` `cat /etc/sysconfig/network/ifcfg-bond0`

`BONDING_MASTER='yes'`

`BONDING_MODULE_OPTS='mode=active-backup miimon=100'`

`BONDING_SLAVE0='eth0'`

`BONDING_SLAVE1='eth1`

`BOOTPROTO='static'`

`BROADCAST=''`

`ETHTOOL_OPTIONS=''`

`IPADDR='192.168.122.140/24'`

`MTU=''`

`NAME=''`

`NETWORK=''`

`REMOTE_IPADDR=''`

`STARTMODE='auto'`

`USERCONTROL='no'`

在这个示例文件中，有一些事情需要注意。首先，`BONDING-SLAVE`行表示哪个网络接口用作从设备。如您所见，在此配置中，有两个接口添加到绑定中。

另一个重要参数是`BONDING_MODULE_OPTS`。这里指定了传递给绑定内核模块的选项。正如你所看到的，模式被设置为`active_backup`，`miimon`参数告诉焊接必须监控焊接接口的频率(以毫秒表示)。如果你想确保你的债券反应快，你可以考虑设置这个参数为 50 毫秒。

如果您已经在绑定配置中指定了`BONDING_SLAVE`行，您不必为分配给绑定设备的接口创建任何配置。只要确保不存在它们的配置文件，绑定就可以工作。也不需要告诉内核加载绑定内核模块。这将在从网络脚本初始化绑定设备时自动加载。

如果您在绑定配置中没有`BONDING_SLAVE`行，您必须为每个预期的从接口修改接口文件。(这些是文件`/etc/sysconfig/network-scripts/ifcfg-eth0`等等。)在清单 3-2 中，您可以看到这个文件的内容应该是什么样子。

清单 3-2。带有绑定配置的示例接口文件

`DEVICE=eth0`

`BOOTPROTO=none`

`ONBOOT=yes`

`USERCTL=no`

`MASTER=bond0`

`SLAVE=yes`

`TYPE=Ethernet`

## 处理多播

网络配置中要考虑的另一部分是多播支持。多播是默认的通信方法，因为它易于设置。对于不能使用多播的环境，也支持单播。在这一章的后面，你将读到如何设置你的集群为单播。在许多网络中，这是一个问题。一般来说，如果所有群集节点都连接到同一个物理交换机，多播就没有问题。在许多网络中，不同的交换机相互连接，形成一个大的广播域。如果是这种情况，您必须采取措施，确保来自一台交换机的多播数据包也转发到所有其他交换机。

要查看的参数是多播侦听(也称为 IGMP 侦听)。IGMP 监听使交换机只将组播数据包转发到那些检测到组播地址的交换机端口。一般来说，这很好，因为这意味着所有其他节点都没有接收到多播数据包。然而，在交换机互连的网络上，这可能会导致问题。由于群集节点默认使用多播进行通信，这将导致群集节点看不到彼此。如果发生这种情况，您可以考虑在交换机上完全关闭多播侦听(虽然这会降低性能)。

如果您的交换机是一个虚拟桥接设备，如 KVM 和 Xen 虚拟化环境中常用的那样，您可以通过更改 sysfs 文件系统中的一个参数来修改`multicast_snooping`行为。在`/sys/class/net`中，每个被配置的网桥都有一个子目录，例如`/sys/class/net/br0`。在这个目录中，您将找到`bridge/multicast_snooping`文件，默认情况下，该文件的值为 1，用于启用多播监听。如果您遇到多播问题，通过将该值回显到文件中，将该文件的值更改为 0。如果这行得通，你也可以尝试值 2，它确实启用了`multicast_snooping`，但在智能模式下，这应该也适用于互连的不同交换机之间。

要自动执行此配置设置，您应该将其包含在引导过程中的某个位置。您可以通过修改`/etc/init.d/boot.local`文件以包含以下脚本行来做到这一点:

`# set multicast_snooping`

`cd /sys/class/net`

`for i in br*`

`do`

`echo 0 > br$i/bridge/multicast_snooping`

`done`

## corosync 还是 cman？

在所有当前的 HA 集群堆栈中，`corosync`是默认解决方案。这意味着您应该在所有情况下使用`corosync`。然而，在某些特定的情况下，`corosync`不起作用。在撰写本文时，Red Hat Enterprise Linux 6 就是这种情况。其中必须使用 cLVM 或 GFS2 文件系统。这种行为有望在 Red Hat Enterprise Linux 7.X 中得到改变。

## 配置协同同步

要创建基于 Corosync 的集群，请确保安装了`corosync`、`pacemaker`和`crmsh`包。在本节中，您将只配置`corosync`，但是它也必须知道资源管理层，这就是为什么您想在安装`corosync`包的同时安装`pacemaker`的原因。为了以后能够管理 Pacemaker 层，此时还需要安装`crmsh`环境。以下过程描述了如何使用`corosync`设置基本集群:

Open the file /`etc/corosync/corosync.conf` in your favorite text editor.   Locate the `bindnetaddr` parameter. This parameter should have as its value the IP address that is used to send the cluster packets. Next, change the `nodeid` parameter. This is the unique ID for this node that is going to be used on the cluster. To avoid any conflicts with auto-generated node IDs, it’s better to manually change the node ID. The last byte of the IP address of this node could be a good choice for the node ID.   Find the `mcastaddr` address. As not all multicast addresses are supported in all situations, make sure the multicast address starts with 224.0.0\. (Yes, really, it makes no sense, but some switches can only work with these addresses!) All nodes in the same cluster require the same multicast address here. If you have several clusters, every cluster needs a unique multicast address. The final result will look like Listing 3-3.  

清单 3-3。示例`corosync.conf`配置文件

`compatibility: whitetank`

`aisexec {`

`user:           root`

`group:          root`

`}`

`service {`

`ver:            0`

`name:           pacemaker`

`use_logd:       yes`

`}`

`totem {`

`version:        2`

`token:          5000`

`token_retransmits_before_loss_const: 10`

`join:           60`

`consensus:      6000`

`vsftype:        none`

`max_messages:   20`

`clear_node_high_bit: yes`

`secauth:        off`

`threads:        0`

`interface {`

`ringnumber: 0`

`bindnetaddr: 192.168.122.130`

`mcastaddr: 239.0.0.95`

`mcastport: 5405`

`ttl: 1`

`}`

`}`

`logging {`

`fileline: off`

`to_stderr: no`

`to_logfile: no`

`to_syslog: yes`

`syslog_facility: daemon`

`debug: off`

`timestamp: off`

`logger_subsys {`

`subsys: AMF`

`debug: off`

`}`

`}`

`amf {`

`mode: disabled`

`}`

If you’re creating the configuration on SUSE, you’ll be fine and won’t need anything else. On Red Hat, you will have to tell `corosync` which resource manager is used. That is because on Red Hat, you might be using `rgmanager` instead. To do this on Red Hat, create a file with the name `/etc/corosync/service.d/pcmk`, and give it the same contents as in Listing 3-4.  

清单 3-4。告诉`corosync`启动起搏器集群管理器

`service {`

`name: pacemaker`

`ver: 1`

`}`

`END`

注意，在清单 3-3 的示例`corosync.conf`配置文件中，已经有一个服务`{ }`部分告诉`corosync`加载`pacemaker`。由于 Red Hat 上的示例配置文件中没有这一部分，所以您可能会忽略它。

Close the configuration file and write the changes. Now, start the `corosync` service. SUSE uses the `openais` service script to do that (that’s for legacy reasons). On Red Hat and related distributions, you can just use `service corosync start` to start the `corosync` service. Also, make sure the service will automatically restart on reboot of the node, using `chkconfig [openais|corosync] on`. On SLES 12 and RHEL 7, you’ll use `systemctl start pacemaker` to start the services and `systemctl enable pacemaker` to make sure it starts automatically.   At this point, you have a one-node cluster. As root, run the `crm_mon` command, to verify that the cluster is operational (see Listing 3-5).  

清单 3-5。使用`crm_mon`验证集群操作

`Last updated: Tue Feb  4 08:42:18 2014`

`Last change: Tue Feb  4 07:41:00 2014 by hacluster via crmd on node2`

`Stack: classic openais (with plugin)`

`Current DC: node2 - partition WITHOUT quorum`

`Version: 1.1.9-2db99f1`

`1 Nodes configured, 2 expected votes`

`0 Resources configured.`

`Online: [ node2 ]`

At this point, you have a one-node cluster. You now have to get the configuration to the other side as well. To do this, use the command `scp /etc/corosync/corosync.conf node1` (in which you need to change the name node1 by the name of your other node). On SLES, the recommended way is to use `hac-cluster-join` from the new node, which copies over `corosync.conf` and sets up SSH and other required parameters.   Open the file `/etc/corosync/corosync.conf` on the second node and change the `nodeid` parameter. Make sure a unique node ID is used, or use automatically configured node IDs (which is the default). Also, change the `bindnetaddr` line. This should reflect the IP network address that `corosync` should bind to (and not the IP address).   Start and enable the `openais` service and run `crm_mon`. You should now see that there are two nodes in the cluster.  

### 了解 corosync.conf 设置

现在您已经建立了第一个集群，让我们看看`corosync.conf`文件中的一些配置部分。第一个重要的部分是服务部分，可以在清单 3-3 中看到。在这一部分，您将告诉`corosync`它应该加载什么。除了将这个配置放在`corosync.conf`中，您还可以将它包含在`/etc/corosync/service.d`目录中。

在清单 3-3 中，您可以看到要加载的服务的名称被设置为`pacemaker`。除此之外，`use_mgmtd`参数用于加载管理守护进程，这是使用遗留`crm_gui`管理工具所需的接口。参数`use_logd`告诉集群拥有自己的日志进程。SLES 和 RHEL 的最新版本不再需要这两个参数。

`corosync.conf`文件的重要部分是图腾部分。在这里，您定义应该如何使用协议。在图腾拓扑中，使用了一个簇环。这个环由所有集群节点组成，这些节点在环上传递一个令牌。`token`参数指定允许令牌传递多长时间，以毫秒表示。因此，默认情况下，令牌有五秒钟的时间在环上传递。

与令牌参数相关的是`token_retransmits_before_loss_const`参数。这是在节点被认为在群集中丢失之前可以丢失的令牌数量。如果在令牌超时期限内没有收到节点的消息，则节点将被视为丢失，因此，默认情况下，在五秒钟后。

下一个重要的部分是接口的声明。在清单 3-4 中，只有一个接口声明只使用一个环。如果您希望群集流量冗余，您可以考虑通过包含第二个接口来建立冗余环。如果你这样做，确保使用一个唯一的组播地址，并给环号码 1。另外，您必须设置`rrp_mode`(冗余环协议模式)。将其设置为激活，以确保两个环都被激活使用。不使用 rrp，在网络接口上使用绑定是一个更好的解决方案，它更容易设置，并且还支持其他服务的冗余。

您还可以包括日志记录部分，以进一步定义如何处理日志记录。清单 3-6 给出了一个示例配置。

清单 3-6。样品`corosync.conf`测井剖面

`logging {`

`fileline: off`

`to_stderr: no`

`to_logfile: yes`

`to_syslog: yes`

`logfile: /var/log/cluster/corosync.log`

`debug: off`

`timestamp: on`

`}`

使用这些不言自明的参数来定义应该如何在集群中处理日志记录。

### 不支持多播的网络

在某些网络上，不支持多播。如果您的网络是这种情况，则上一节中描述的过程不起作用。您必须创建一个基于 UDPU 协议配置的配置，才能让它工作。与之前描述的配置最相关的差异如下:

*   在接口部分，您必须包括允许作为集群成员的所有节点的地址。
*   您不再需要多播地址。

在清单 3-7 中，您可以看到一个典型的单播集群配置是什么样子的。

清单 3-7。单播`corosync`集群配置

`aisexec {`

`group:  root`

`user:   root`

`}`

`service {`

`use_mgmtd:      yes`

`use_logd:       yes`

`ver:    0`

`name:   pacemaker`

`}`

`totem {`

`rrp_mode:       none`

`join:   60`

`max_messages:   20`

`vsftype:        none`

`transport:      udpu`

`nodeid: 145`

`consensus:      6000`

`secauth:        off`

`token_retransmits_before_loss_const:    10`

`token:  5000`

`version:        2`

`interface {`

`bindnetaddr:    192.168.1.0`

`member {`

`memberaddr:     192.168.1.144`

`}`

`member {`

`memberaddr:     192.168.1.145`

`}`

`mcastport:      5405`

`ringnumber:     0`

`}`

`clear_node_high_bit:    no`

`}`

`logging {`

`to_logfile:     no`

`to_syslog:      yes`

`debug:  off`

`timestamp:      off`

`to_stderr:      no`

`fileline:       off`

`syslog_facility:        daemon`

`}`

`amf {`

`mode:   disable`

`}`

清单 3-7 中有两个配置参数需要进一步解释。同样在单播模式下，可以使用冗余环(但考虑使用绑定)。如果您想在所有节点上保持`corosync.conf`文件的内容一致，您可以考虑使用自动生成的节点 id。

## 配置 cman

如前所述，`corosync`应该是您用来实现成员层的默认解决方案。因为你将很难在 Red Hat 6 中使用 cLVM 和 GFS2 共享存储的`corosync`。x，有时，你也可能不得不在成员层使用`cman`。以下过程描述了如何做到这一点:

Install required software.  

`yum install -y cman gfs2-utils gfs2-cluster`

Edit `/etc/sysconfig/cman` and make sure it includes the following line:  

`CMAN_QUORUM_TIMEOUT=0`

Create the file `/etc/cluster/cluster.conf` with the following contents (make sure to replace node names). Note that it does include a fencing “dummy.” `cman` must be able to fence nodes, but if that happens, it must send the fencing instruction to the Pacemaker layer (fencing is discussed in depth in [Chapter 5](05.html)):  

`<?xml version="1.0"?>`

`<cluster config_version="1" name="mysql-cluster">`

`<logging debug="off"/>`

`<clusternodes>`

`<clusternode name="mysql1.moodle.hosting.local" nodeid="1">`

`<fence>`

`<method name="pcmk-redirect">`

`<device name="pcmk" port="mysql1.moodle.hosting.local"/>`

`</method>`

`</fence>`

`</clusternode>`

`<clusternode name="mysql2.moodle.hosting.local" nodeid="2">`

`<fence>`

`<method name="pcmk-redirect">`

`<device name="pcmk" port="mysql2.moodle.hosting.local"/>`

`</method>`

`</fence>`

`</clusternode>`

`</clusternodes>`

`<fencedevices>`

`<fencedevice name="pcmk" agent="fence_pcmk"/>`

`</fencedevices>`

`</cluster>`

Run `ccs_config_validate` to validate the configuration.   Start `cman` and `pacemaker` services on both nodes, as follows:  

`service cman start; service pacemaker start`

Put both services in the runlevels, as follows:  

`chkconfig cman on; chkconfig pacemaker on`

Use `cman_tool` nodes to verify availability of the nodes.   Use `crm_mon` to verify availability of the resources.   Restart both nodes and use `cman_tool` nodes to verify that all comes up.  

## 摘要

在本章中，您已经学习了如何创建集群成员层。您首先阅读了如何设置网络绑定以在网络级别添加保护。接下来，您将阅读如何确保多播在您的环境中顺利工作。接下来你会读到如何在多播或单播模式下设置`corosync`。本章的最后一部分致力于在 Red Hat 环境中安装`cman`。在下一章中，您将了解更多关于 Pacemaker 的组织和管理方式。